{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2962cb4",
   "metadata": {},
   "source": [
    "#### 0. Pre-Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f67f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import logging\n",
    "from typing import Literal\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import wkt\n",
    "import fiona\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import utility constants and functions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd71cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # or DEBUG\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2abf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger with time capture\n",
    "from functools import wraps\n",
    "\n",
    "def log_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logger.info(f\"Starting: {func.__name__}\")\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        logger.info(f\"Completed: {func.__name__} in {end - start:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1919ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants here\n",
    "geo_crs = \"EPSG:4326\"\n",
    "projected_crs = \"EPSG:3857\" \n",
    "POC_FINALIZED_COUNTIES = [\n",
    "    # urban\n",
    "    '17031',\n",
    "    '13121',\n",
    "    '53033',\n",
    "    # sub-urban\n",
    "    '48491',\n",
    "    '29181',\n",
    "    '42011',\n",
    "    # rural \n",
    "    '55107',\n",
    "    '35051',\n",
    "    '17127'\n",
    "]\n",
    "\n",
    "ENCUMBRANCES = [\n",
    "    'roadways',\n",
    "    'railways',\n",
    "    'protected_lands',\n",
    "    'wetlands',\n",
    "    'transmission_lines',\n",
    "    ]\n",
    "EncumbranceType = Literal[\n",
    "    'roadways',\n",
    "    'railways',\n",
    "    'protected_lands',\n",
    "    'wetlands',\n",
    "    'transmission_lines',\n",
    "]\n",
    "\n",
    "# Parquet formats are available\n",
    "# Files are saved as {fips_encumbrance.parquet} or {fips_parcels.parquet} \n",
    "PARQUET_FOLDER = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e0fc4",
   "metadata": {},
   "source": [
    "#### 1. Ingestion: Encumbrance and Parcel data from pre-processed parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e0668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get encumbrance parquet for the county\n",
    "def load_encumbrance_data(\n",
    "        fips_code:str,\n",
    "        encumbrance:str):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Load encumbrance data saved in local\n",
    "    gdf_encumbrance = gpd.read_parquet(os.path.join(PARQUET_FOLDER, f\"{fips_code}_{encumbrance}.parquet\"))\n",
    "    \n",
    "    # Convert to EPSG:4326\n",
    "    gdf_encumbrance = gdf_encumbrance.to_crs(geo_crs)\n",
    "    print(f'CRS of the {encumbrance} dataframe is {gdf_encumbrance.crs}')\n",
    "    return gdf_encumbrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6cba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get parcel data for the defined county\n",
    "def load_parcel_data(fips_code: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load parcel data from BigQuery and filter by FIPS code.\n",
    "    \"\"\"\n",
    "    # Load parcel parquet file saved in local\n",
    "    gdf_parcel = gpd.read_parquet(os.path.join(PARQUET_FOLDER, f\"{fips_code}_parcels.parquet\"))\n",
    "    \n",
    "    # Convert to EPSG:4326\n",
    "    gdf_parcel = gdf_parcel.to_crs(geo_crs)\n",
    "    print(f'CRS of the parcel dataframe is {gdf_parcel.crs}')\n",
    "    return gdf_parcel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73fe9d",
   "metadata": {},
   "source": [
    "#### 2. Calculating Proximity and Intersection Metrics\n",
    "*Intersecting all parcels with buffered values of encumbrance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f67024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define buffer distances and scores based on polygon or line geometry\n",
    "def buffer_scores_and_labels(\n",
    "        encumbrance: EncumbranceType):\n",
    "    '''\n",
    "    Define buffer distances and score labels based on encumbrance type.\n",
    "    '''\n",
    "    if encumbrance == 'railways' or encumbrance == 'transmission_lines':\n",
    "        buffer_distances = [5, 150, 300, 750, 1000]\n",
    "        score_labels = ['intersects', 'very high', 'high', 'medium', 'low']\n",
    "    \n",
    "    elif encumbrance == 'roadways':\n",
    "        buffer_distances = [5, 10, 25, 50, 100]\n",
    "        score_labels = ['intersects', 'very high', 'high', 'medium', 'low']\n",
    "    \n",
    "    elif encumbrance == 'wetlands' or encumbrance == 'protected_lands':\n",
    "        buffer_distances = [0, 5, 10, 50, 100]\n",
    "        score_labels = ['intersects', 'very high', 'high', 'medium', 'low']\n",
    "\n",
    "    return buffer_distances, score_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355ad3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate intersection metrics\n",
    "def calculate_intersection_metrics(\n",
    "        encumbrance:EncumbranceType,\n",
    "        all_parcels,\n",
    "        matched_parcels,\n",
    "        buffered_encumbrance\n",
    "):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # Reset index to make sure merge works properly\n",
    "    buffer_gdf = buffered_encumbrance.reset_index().rename(columns={'index': 'buffer_index'})\n",
    "\n",
    "    # Merge buffer geometries into matched results using index_right from sjoin\n",
    "    matched_with_geom = matched_parcels.reset_index().merge(\n",
    "        buffer_gdf[['buffer_index', 'geometry']],\n",
    "        left_on='index_right',\n",
    "        right_on='buffer_index',\n",
    "        how='left',\n",
    "        suffixes=('', '_buffer')\n",
    "    )\n",
    "    # Ensure both geometries are in a projected CRS (3857) for accurate area\n",
    "    # Setting projection for parcel geometry\n",
    "    matched_with_geom = matched_with_geom.set_geometry('geometry')\n",
    "    matched_with_geom = matched_with_geom.to_crs(projected_crs)\n",
    "    \n",
    "    # Setting projection for buffered value of encumbrance geometry \n",
    "    matched_with_geom = matched_with_geom.set_geometry('geometry_buffer', drop=False)\n",
    "    matched_with_geom[f'geometry_buffer_{encumbrance}'] = matched_with_geom['geometry_buffer'].to_crs(projected_crs)\n",
    "\n",
    "    # Calculate intersection geometry \n",
    "    matched_with_geom[f'intersection_geom_{encumbrance}'] = matched_with_geom['geometry'].intersection(matched_with_geom[f'geometry_buffer_{encumbrance}'])\n",
    "\n",
    "    # Group by parcel index -- this will help later to pick parcel row in case of multiple intersections\n",
    "    matched_with_geom['orig_index'] = matched_with_geom['index']  # preserve original index for reassigning\n",
    "\n",
    "    # Calculate intersection metrics for lines\n",
    "    if encumbrance in ['railways', 'roadways', 'transmission_lines']:\n",
    "        \n",
    "        # Calculate intersection perimeter\n",
    "        matched_with_geom[f'intersec_perim_{encumbrance}'] = matched_with_geom[f'intersection_geom_{encumbrance}'].length\n",
    "\n",
    "        # Approximate true line length (perimeter / 2)\n",
    "        matched_with_geom[f'approx_line_len_{encumbrance}'] = round(\n",
    "            matched_with_geom[f'intersec_perim_{encumbrance}'] / 2, 2\n",
    "        )\n",
    "\n",
    "        # Retain row with max approximate line length\n",
    "        max_length_idx = matched_with_geom.groupby('orig_index')[f'approx_line_len_{encumbrance}'].idxmax()\n",
    "        matched_max_area = matched_with_geom.loc[max_length_idx]\n",
    "\n",
    "        # Store this back into parcels dataframe\n",
    "        all_parcels.loc[matched_max_area['orig_index'], f'approx_line_len_{encumbrance}'] = matched_max_area[f'approx_line_len_{encumbrance}'].values\n",
    "\n",
    "    elif encumbrance in ['wetlands', 'protected_lands']:\n",
    "\n",
    "        # Calculate parcel intersection ratio\n",
    "        matched_with_geom[f'intersec_area_{encumbrance}'] = round(matched_with_geom[f'intersection_geom_{encumbrance}'].area,2)\n",
    "        matched_with_geom['parcel_area'] = matched_with_geom['geometry'].area\n",
    "        matched_with_geom[f'area_ratio_{encumbrance}'] = round((\n",
    "            matched_with_geom[f'intersec_area_{encumbrance}'] / matched_with_geom['parcel_area']\n",
    "        ),2)\n",
    "        \n",
    "        # Calculate parcel centroid to wetland distance\n",
    "        # Convert parcel centroid column to GeoSeries\n",
    "        # Convert WKT strings to actual shapely geometries\n",
    "        matched_with_geom['centroid'] = matched_with_geom['centroid'].apply(wkt.loads)\n",
    "        matched_with_geom['centroid'] = gpd.GeoSeries(matched_with_geom['centroid'], crs=geo_crs)\n",
    "        \n",
    "        # Reproject centroid to projected CRS (e.g. 3857)\n",
    "        matched_with_geom['centroid'] = matched_with_geom['centroid'].to_crs(projected_crs)\n",
    "\n",
    "        # parcel centroid to encumbered geometry distance\n",
    "        matched_with_geom[f'parcel_dist_to_{encumbrance}'] = round(matched_with_geom['centroid'].distance(\n",
    "            matched_with_geom[f'geometry_buffer_{encumbrance}']\n",
    "            ),2)\n",
    "        \n",
    "        # Chunk to retain row with the max area ratio\n",
    "        max_area_idx = matched_with_geom.groupby('orig_index')[f'area_ratio_{encumbrance}'].idxmax()\n",
    "        matched_max_area = matched_with_geom.loc[max_area_idx]\n",
    "\n",
    "        # Store metrics back in parcels_mod for max intersection row\n",
    "        for col in ['intersec_area', 'area_ratio', 'parcel_dist_to']:\n",
    "            all_parcels.loc[matched_max_area['orig_index'], f'{col}_{encumbrance}'] = matched_max_area[f'{col}_{encumbrance}'].values\n",
    "\n",
    "    # Calculate number of intersecting encumbrances per parcel (for all encumbrance types)\n",
    "    intersection_counts = matched_with_geom.groupby('orig_index').size()\n",
    "    \n",
    "    # Store the number of intersections as a separate metric\n",
    "    all_parcels.loc[intersection_counts.index, f'n_{encumbrance}_intersections'] = intersection_counts.values\n",
    "    \n",
    "    logger.info(f\"Finished calculating intersection metrics for encumbrance {encumbrance}!\")\n",
    "    return all_parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proximity score and intersection metrics based on encumbrance type\n",
    "@log_time\n",
    "def get_proximity_score_and_intersection_metrics(\n",
    "        encumbrance: EncumbranceType,\n",
    "        gdf_parcel: gpd.GeoDataFrame, \n",
    "        gdf_encumbrance: gpd.GeoDataFrame, \n",
    "        ) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Assign proximity scores to parcels based on their distance to encumbrance features.\n",
    "    \n",
    "    Parameters:\n",
    "    parcels (GeoDataFrame): The parcels to be scored.\n",
    "    encumbrance (GeoDataFrame): The encumbrance features to score against.\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: Parcels with assigned proximity scores.\n",
    "    \"\"\"\n",
    "    # Start logging process\n",
    "    logger.info(\"Starting proximity scoring...\")\n",
    "\n",
    "    # Derive scores based on nature of encumbrance\n",
    "    buffer_distances, score_labels = buffer_scores_and_labels(encumbrance)\n",
    "    logger.info(f\"Buffer distances: {buffer_distances}\")\n",
    "    logger.info(f\"Score labels: {score_labels}\")\n",
    "    \n",
    "    # Ensure the CRS of both GeoDataFrames match\n",
    "    gdf_encumbrance = gdf_encumbrance.to_crs(gdf_parcel.crs)\n",
    "\n",
    "    # Create a copy of the parcels GeoDataFrame to avoid modifying the original\n",
    "    parcels_mod = gdf_parcel.copy()\n",
    "\n",
    "    # Initialize a new column for proximity score\n",
    "    # Explicitly defining dtype object to avoid SettingWithCopyWarning\n",
    "    parcels_mod[f'proximity_score_{encumbrance}'] = pd.Series([None]*len(parcels_mod), dtype='object')\n",
    "\n",
    "    # Calculate proximity scores based on distance to encumbrance features\n",
    "    # Initialize i \n",
    "    i = 0\n",
    "    for distance, label in zip(buffer_distances, score_labels):\n",
    "        \n",
    "        # Buffer individual geometries by specified distance\n",
    "        # Project to a projected CRS for buffering\n",
    "        buffer_gdf = gdf_encumbrance.to_crs(projected_crs)\n",
    "\n",
    "        # Create buffered geometries and assign to a new column\n",
    "        buffer_gdf['buffered_geometry'] = buffer_gdf.geometry.buffer(distance) # This geometry holds the buffered version\n",
    "\n",
    "        # Drop the original geometry column\n",
    "        buffer_gdf = buffer_gdf.drop(columns=['geometry'])\n",
    "\n",
    "        # Rename the buffered geometry column to 'geometry'\n",
    "        buffer_gdf = buffer_gdf.rename(columns={'buffered_geometry': 'geometry'})\n",
    "\n",
    "        # Set CRS and reproject back to geo_crs\n",
    "        buffer_gdf = buffer_gdf.set_geometry('geometry').to_crs(geo_crs)\n",
    "        logger.info(f'Created buffered geometry with distance {distance} meters and CRS {buffer_gdf.crs}')\n",
    "        \n",
    "        # Use spatial join to find parcels within the buffer distance\n",
    "        matched = gpd.sjoin(parcels_mod[parcels_mod[f'proximity_score_{encumbrance}'].isna()],\n",
    "                             buffer_gdf,\n",
    "                             predicate='intersects',\n",
    "                             how='inner')\n",
    "\n",
    "        # Assign labels to matched parcels based on proximity score\n",
    "        # Use the index of matched parcels to assign the proximity score\n",
    "        parcels_mod.loc[matched.index, f'proximity_score_{encumbrance}'] = label\n",
    "        \n",
    "        # Add encumbrance column values to main dataframe\n",
    "        # TODO: Find more elegant solution\n",
    "        cols_to_add = buffer_gdf.columns.difference(['geometry'])\n",
    "        for col in cols_to_add:\n",
    "            parcels_mod.loc[matched.index, f\"{col.lower()}_{encumbrance}\"] = matched[col].values\n",
    "        logger.info(f\"Assigned proximity score {label} to {len(matched)} parcels...\")\n",
    "\n",
    "        # Add intersecton metrics when buffer is smallest\n",
    "        if i == 0:\n",
    "            logger.info('Now adding intersection metrics...')\n",
    "            parcels_mod = calculate_intersection_metrics(\n",
    "                encumbrance=encumbrance,\n",
    "                all_parcels=parcels_mod,\n",
    "                matched_parcels=matched,\n",
    "                buffered_encumbrance=buffer_gdf\n",
    "            )\n",
    "        # Increment i\n",
    "        i += 1 \n",
    "            \n",
    "    # Fill remaining as no encumbrance and change to geographic CRS\n",
    "    parcels_mod.fillna({f'proximity_score_{encumbrance}':'no_encumbrance'}, inplace=True)\n",
    "    \n",
    "    # Re-project everything to geographic CRS\n",
    "    for column in parcels_mod.select_dtypes(include=['geometry']).columns:\n",
    "        parcels_mod[column] = parcels_mod[column].to_crs(geo_crs)\n",
    "    print(f'CRS of output dataframe is {parcels_mod.crs}')\n",
    "    print('Proximity scoring complete! Counts of proximity scores are...')\n",
    "\n",
    "    # Print value counts of proximity scores\n",
    "    print(parcels_mod[f'proximity_score_{encumbrance}'].value_counts())\n",
    "    return parcels_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba048adc",
   "metadata": {},
   "source": [
    "#### 3. Calculating Intersection Strength (For Polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c70f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate intersection strength score\n",
    "def calculate_intersection_score(\n",
    "        encumbrance:EncumbranceType,\n",
    "        gdf_parcel: gpd.GeoDataFrame,\n",
    "        *,\n",
    "        area_ratio_weight: float = 0.5,\n",
    "        dist_weight: float = 0.3,\n",
    "        n_intersections_weight: float = 0.2,\n",
    "        area_ratio_thresholds: tuple = (0.4, 0.9),\n",
    "        dist_thresholds: tuple = (100, 50, 10, 0),\n",
    "        n_intersections_thresholds: tuple = (2 ,3),\n",
    "        score_thresholds: tuple = (0.35, 0.7)\n",
    "    ) -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    Calculates intersection strength score for a given encumbrance using:\n",
    "    - Area ratio\n",
    "    - Centroid distance\n",
    "    - Number of intersections\n",
    "    \n",
    "    Accepts tunable weights and thresholds via kwargs.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame with a new 'intersection_score_{encumbrance}' column.\n",
    "    '''\n",
    "    # Check score is only asked for polygon encumbrances\n",
    "    if encumbrance not in ['wetlands', 'protected_lands']:\n",
    "        raise ValueError(\n",
    "            f\"Intersection scoring is only applicable for polygon encumbrances. \"\n",
    "            f\"Valid options are: 'wetlands', 'protected_lands'.\"\n",
    "        )\n",
    "\n",
    "    # Create dataframe copy to avoid modifying the original\n",
    "    parcels_mod = gdf_parcel.copy()\n",
    "\n",
    "    # Unpack thresholds\n",
    "    ar_low, ar_high = area_ratio_thresholds\n",
    "    dist_low, dist_med, dist_high, dist_overwrite = dist_thresholds\n",
    "    nint_med, nint_high = n_intersections_thresholds\n",
    "\n",
    "    # Score area_ratio\n",
    "    ar_col = f'area_ratio_{encumbrance}'\n",
    "    parcels_mod[f'score_ar_{encumbrance}'] = parcels_mod[ar_col].apply(\n",
    "        lambda x: 1 if x >= ar_high else (0.5 if x >= ar_low else 0.25 if x > 0 else 0)\n",
    "    )\n",
    "\n",
    "    # Score parcel_dist_to\n",
    "    dist_col = f'parcel_dist_to_{encumbrance}'\n",
    "    parcels_mod[f'score_dist_{encumbrance}'] = parcels_mod[dist_col].apply(\n",
    "        lambda x: 1 if x <= dist_high else (0.5 if x <= dist_med else 0.25 if x <= dist_low else 0.15 if x > 0 else 0)\n",
    "    )\n",
    "\n",
    "    # Score number of intersections\n",
    "    nint_col = f'n_{encumbrance}_intersections'\n",
    "    parcels_mod[f'score_nint_{encumbrance}'] = parcels_mod[nint_col].apply(\n",
    "        lambda x: 1 if x >= nint_high else (0.5 if x == nint_med else 0.25 if x > 0 else 0)\n",
    "    )\n",
    "\n",
    "    # Final weighted score\n",
    "    score_col = f'intersection_score_{encumbrance}'\n",
    "    parcels_mod[score_col] = (\n",
    "        parcels_mod[f'score_ar_{encumbrance}'] * area_ratio_weight +\n",
    "        parcels_mod[f'score_dist_{encumbrance}'] * dist_weight +\n",
    "        parcels_mod[f'score_nint_{encumbrance}'] * n_intersections_weight\n",
    "    )\n",
    "\n",
    "    # TODO: Once testing is over, drop intermediate columns\n",
    "    # parcels_mod = parcels_mod.drop(columns=[\n",
    "    # f'parcel_dist_to_{encumbrance}',\n",
    "    # f'area_ratio_{encumbrance}',\n",
    "    # f'n_{encumbrance}_intersections'])\n",
    "\n",
    "    # Add low, medium, high labels\n",
    "    # Labeling with override for high-impact flags\n",
    "    # TODO: Make this more efficient\n",
    "    low_thres, high_thres = score_thresholds\n",
    "    label_col = f'intersection_label_{encumbrance}'\n",
    "\n",
    "    parcels_mod[label_col] = parcels_mod.apply(\n",
    "    lambda row: None if row[score_col] == 0 else (\n",
    "        'high' if (\n",
    "            row[ar_col] >= ar_high \n",
    "            or row[dist_col] == dist_overwrite\n",
    "        ) else (\n",
    "            'low' if row[score_col] < low_thres else\n",
    "            'medium' if row[score_col] < high_thres else\n",
    "            'high'\n",
    "        )\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "    print(f'Intersection scoring completed for {encumbrance}!')\n",
    "    return parcels_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9328382",
   "metadata": {},
   "source": [
    "#### 4. Calculating Summary Stats Using Calculated Metrics\n",
    "**Questions from a Product perspective:**\n",
    "1. What are the top x land uses for parcels in a given proximity bucket?\n",
    "2. Who are the top x owners in a given proximity bucket?\n",
    "\n",
    "3. What do the total parcel value quartiles look like across proximity buckets?\n",
    "4. What is the quartile values for sq. footage across proximity buckets?\n",
    "5. What are quartile values for value/sq.ft. across proximity buckets?\n",
    "6. What % of parcels in a given proximity bucket are taxable?\n",
    "7. For taxable parcels, what are the quartile values for tax amount?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c34198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot quartile values for a given measure and proximity labels\n",
    "def plot_measure_quartiles(\n",
    "        df, \n",
    "        grouping_col, \n",
    "        labels = ['low', 'medium', 'high'],\n",
    "        measures= ['tot_val', 'land_sq_ft', 'val_per_sq_ft','tax_amt', 'tax_per_sq_ft'],\n",
    "        quantiles = [0.25, 0.50, 0.75, 0.90, 0.95]\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plots Q25, Q50, Q75 lines for each measure and proximity label.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Raw parcel data with proximity_score and measure columns.\n",
    "        labels (list of str): e.g., ['high', 'medium']\n",
    "        measures (list of str): e.g., ['tot_val', 'land_sq_ft']\n",
    "    \"\"\"\n",
    "    # Filter for selected proximity labels\n",
    "    df_new = df[df[grouping_col].isin(labels)]\n",
    "\n",
    "    for measure in measures:\n",
    "\n",
    "        # Copying dataframe to avoid warnings\n",
    "        df_filtered = df_new.copy()\n",
    "        \n",
    "        if measure == 'val_per_sq_ft':\n",
    "\n",
    "            # Create a mask for valid values\n",
    "            valid_mask = df_filtered['tot_val'].notna() & df_filtered['land_sq_ft'].notna() & (df_filtered['land_sq_ft'] != 0)\n",
    "\n",
    "            # Create the new column safely\n",
    "            df_filtered.loc[valid_mask, 'val_per_sq_ft'] = df_filtered.loc[valid_mask, 'tot_val'] / df_filtered.loc[valid_mask, 'land_sq_ft']\n",
    "\n",
    "            # Round to two decimal points\n",
    "            df_filtered['val_per_sq_ft'] = df_filtered['val_per_sq_ft'].round(2)\n",
    "        \n",
    "        elif measure == 'tax_per_sq_ft':\n",
    "            # Create a mask for valid values\n",
    "            valid_mask = df_filtered['tax_amt'].notna() & df_filtered['land_sq_ft'].notna() & (df_filtered['land_sq_ft'] != 0)\n",
    "\n",
    "            # Create the new column safely\n",
    "            df_filtered.loc[valid_mask, 'tax_per_sq_ft'] = df_filtered.loc[valid_mask, 'tax_amt'] / df_filtered.loc[valid_mask, 'land_sq_ft']\n",
    "\n",
    "            # Round to two decimal points\n",
    "            df_filtered['tax_per_sq_ft'] = df_filtered['tax_per_sq_ft'].round(2)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10,4))\n",
    "\n",
    "        for label in labels:\n",
    "            group = df_filtered[df_filtered[grouping_col] == label]\n",
    "\n",
    "            # Drop missing values for the measure\n",
    "            values = group[measure].dropna()\n",
    "\n",
    "            if values.empty:\n",
    "                continue\n",
    "\n",
    "            # Compute quantiles dynamically\n",
    "            q_vals = values.quantile(quantiles).values\n",
    "            q_labels = [f\"Q{int(q*100)}\" for q in quantiles]\n",
    "\n",
    "            # Plot the values\n",
    "            plt.plot(q_labels, q_vals, marker='o', label=label)\n",
    "\n",
    "        plt.title(f\"Quantile Trends for {measure.replace('_', ' ').title()}\")\n",
    "        plt.xlabel(\"Quantile\")\n",
    "        plt.ylabel(measure.replace('_', ' ').title())\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.legend(title=f\"{grouping_col} Labels\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76605163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization to display top x land_use codes/owners for a given proximity bucket\n",
    "def plot_top_x(\n",
    "        df, \n",
    "        grouping_col,\n",
    "        proximity_label,\n",
    "        use_case='land_use',\n",
    "        top_x=10):\n",
    "    \"\"\"\n",
    "    Plots top X land use codes for a given proximity bucket.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with columns ['proximity_score', 'land_use_t', 'count']\n",
    "        proximity_label (str): e.g., 'high', 'medium', 'low', 'no_encumbrance', etc.\n",
    "        use_case should ideally be either land_use_t or owner\n",
    "        top_x (int): number of top land use codes to show\n",
    "    \"\"\"\n",
    "    # Filter the relevant proximity bucket\n",
    "    subset = df[df[grouping_col] == proximity_label]\n",
    "\n",
    "    if subset.empty:\n",
    "        print(f\"No data found for proximity score: {proximity_label}\")\n",
    "        return\n",
    "    \n",
    "    if use_case == 'land_use':\n",
    "        aggregation = 'land_use_t'\n",
    "    elif use_case == 'owner':\n",
    "        aggregation = 'owner'\n",
    "    \n",
    "    # Get land use counts\n",
    "    land_use_counts = (\n",
    "    subset.groupby(aggregation)\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count', ascending=False)\n",
    "    )\n",
    "\n",
    "    # Get top X land uses\n",
    "    top_land_uses = land_use_counts.nlargest(top_x, 'count')\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(\n",
    "        data=top_land_uses,\n",
    "        x='count',\n",
    "        y=aggregation,\n",
    "        #palette='viridis'\n",
    "    )\n",
    "    plt.title(f\"Top {top_x} Land Use Types for Proximity: {proximity_label}\")\n",
    "    plt.xlabel(\"Parcel Count\")\n",
    "    plt.ylabel(\"Land Use Type\")\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
