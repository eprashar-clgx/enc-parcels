{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dac2ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "from typing import Literal\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import wkt\n",
    "from shapely.geometry import shape\n",
    "from shapely.validation import explain_validity\n",
    "import fiona\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from pandas_gbq import to_gbq\n",
    "from google.cloud import storage\n",
    "\n",
    "# Import utility constants and functions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce1236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global constants here\n",
    "POC_FINALIZED_COUNTIES = [\n",
    "    # urban\n",
    "    '17031',\n",
    "    '13121',\n",
    "    '53033',\n",
    "    # sub-urban\n",
    "    '48491',\n",
    "    '29181',\n",
    "    '42011',\n",
    "    # rural \n",
    "    '55107',\n",
    "    '35051',\n",
    "    '17127',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c9d0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "#POC_DATASET = 'encumbered_parcels'\n",
    "#POC_TABLE = 'base_parcels_poc_counties_0419'\n",
    "geo_crs = \"EPSG:4326\"\n",
    "projected_crs = \"EPSG:3857\" \n",
    "ENCUMBRANCES = [\n",
    "    'roadways',\n",
    "    'railways',\n",
    "    'protected_lands',\n",
    "    'wetlands',\n",
    "    'transmission_lines',\n",
    "    ]\n",
    "EncumbranceType = Literal[\n",
    "    'roadways',\n",
    "    'railways',\n",
    "    'protected_lands',\n",
    "    'wetlands',\n",
    "    'transmission_lines',\n",
    "]\n",
    "\n",
    "LOCAL_DATA_FOLDER = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\"\n",
    "COUNTY_DATA = r\"counties\\tl_2024_us_county\\tl_2024_us_county.shp\"\n",
    "RAILWAYS_DATA = r\"NTAD_North_American_Rail_Network_Lines\\NARN.gdb\" \n",
    "TRANSMISSION_LINES_DATA = r\"transmission_lines\\Transmission_Lines.shp\"\n",
    "ROADWAYS_DATA = r\"NTAD_North_American_Roads\\North_American_Roads.shp\"\n",
    "PROTECTED_LANDS = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\protected_lands\"\n",
    "WETLANDS = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\Wetlands\"\n",
    "WETLAND_ATTRIBUTES = r\"Wetlands\\NWI-Code-Definitions\\NWI-Code-Definitions\\NWI_Code_Definitions.gdb\"\n",
    "PARQUET_INGESTION_PATH = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6a568",
   "metadata": {},
   "source": [
    "#### 1. Cleaning and saving source encumbrance data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fae1d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make valid geometries\n",
    "def fix_invalid_geometries(gdf):\n",
    "    # Iterate over each row and fix invalid geometries\n",
    "    for idx, row in gdf.iterrows():\n",
    "        geom = shape(row['geometry'])  # Convert WKT to Shapely geometry\n",
    "        if not geom.is_valid:\n",
    "            print(f\"Fixing invalid geometry at index {idx}\")\n",
    "            gdf.at[idx, 'geometry'] = geom.buffer(0)  # Makes the geometry valid\n",
    "        else:\n",
    "            gdf.at[idx, 'geometry'] = geom\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7771297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add wetland attributes\n",
    "def add_wetland_attributes(gdf_wetland):\n",
    "    \"\"\"\n",
    "    Load wetland attributes from a geodatabase and return as a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    # Read the geodatabase\n",
    "    gdf_wetland_attributes = gpd.read_file(os.path.join(LOCAL_DATA_FOLDER, WETLAND_ATTRIBUTES))\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        'SYSTEM', 'SYSTEM_NAME', 'SYSTEM_DEFINITION',\n",
    "        'SUBSYSTEM', 'SUBSYSTEM_DEFINITION',\n",
    "        'CLASS', 'CLASS_DEFINITION',\n",
    "        'SUBCLASS', 'SUBCLASS_DEFINITION',\n",
    "        'SPLIT_CLASS', 'SPLIT_CLASS_DEFINITION',\n",
    "        'SPLIT_SUBCLASS', 'SPLIT_SUBCLASS_NAME', 'SPLIT_SUBCLASS_DEFINITION',\n",
    "        'WATER_REGIME', 'WATER_REGIME_DEFINITION',\n",
    "        'MODIFIER1', 'MODIFIER1_NAME', 'MODIFIER1_GROUP', 'MODIFIER1_SUBGROUP', 'MODIFIER1_DEFINITION',\n",
    "        'MODIFIER2', 'MODIFIER2_NAME', 'MODIFIER2_GROUP', 'MODIFIER2_SUBGROUP', 'MODIFIER2_DEFINITION',\n",
    "        'geometry'\n",
    "    ]\n",
    "    gdf_wetland_attributes = gdf_wetland_attributes.drop(\n",
    "        columns=[col for col in columns_to_drop if col in gdf_wetland_attributes.columns])\n",
    "\n",
    "    # Join datasets using the 'ATTRIBUTE' column\n",
    "    wetlands_with_attributes = gdf_wetland.merge(\n",
    "        gdf_wetland_attributes,\n",
    "        how='left',\n",
    "        on='ATTRIBUTE'\n",
    "    )\n",
    "\n",
    "    return wetlands_with_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "093531ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global config variable to store dataset configurations\n",
    "# Config includes paths, read arguments, and cleanup functions for each dataset\n",
    "DATASET_CONFIG = {\n",
    "    'transmission_lines': {\n",
    "        'path': TRANSMISSION_LINES_DATA,\n",
    "        'read_kwargs': {},\n",
    "        'requires_state': False,\n",
    "        'cleanup': lambda gdf: gdf\n",
    "            .drop(columns=[\n",
    "                'OBJECTID', 'SOURCE', 'SOURCEDATE', 'VAL_METHOD', 'VOLTAGE',\n",
    "                'INFERRED', 'SUB_1', 'SUB_2'\n",
    "            ])\n",
    "            .assign(Shape__Len=gdf['Shape__Len'].round(2))\n",
    "    },\n",
    "    'railways': {\n",
    "        'path': RAILWAYS_DATA,\n",
    "        'read_kwargs': {'layer': 'North_American_Rail_Network_Lines'},\n",
    "        'requires_state': False,\n",
    "        'cleanup': lambda gdf: gdf\n",
    "            .drop(columns=[\n",
    "                'FRFRANODE', 'TOFRANODE', 'STFIPS', 'CNTYFIPS', 'STATEAB', 'COUNTRY',\n",
    "                'FRADISTRCT', 'RROWNER1', 'RROWNER2', 'RROWNER3',\n",
    "                'TRKRGHTS1', 'TRKRGHTS2', 'TRKRGHTS3', 'TRKRGHTS4', 'TRKRGHTS5',\n",
    "                'TRKRGHTS6', 'TRKRGHTS7', 'TRKRGHTS8', 'TRKRGHTS9', 'DIVISION',\n",
    "                'SUBDIV', 'BRANCH', 'YARDNAME', 'PASSNGR', 'STRACNET', 'TRACKS',\n",
    "                'NET', 'MILES', 'TIMEZONE', 'SHAPE_Length'\n",
    "            ], errors='ignore')\n",
    "            .assign(KM=gdf['KM'].round(2))\n",
    "    },\n",
    "    'roadways': {\n",
    "        'path': ROADWAYS_DATA,\n",
    "        'read_kwargs': {},\n",
    "        'requires_state': False,\n",
    "        'cleanup': lambda gdf: gdf[gdf['COUNTRY'] == 2]\n",
    "            .drop(columns=['DIR', 'LINKID', 'JURISCODE', 'ROADNUM', 'CLASS', 'NHS'], errors='ignore')\n",
    "    },\n",
    "    'wetlands': {\n",
    "        'gdb_config': lambda state: {\n",
    "            'folder': WETLANDS,\n",
    "            'subfolder': f\"{state}_geodatabase_wetlands\",\n",
    "            'gdb_name': f\"{state}_geodatabase_wetlands.gdb\"\n",
    "        },\n",
    "        'requires_state': True,\n",
    "        'cleanup':  lambda gdf: gdf.drop(columns=['NWI_ID'], errors='ignore'),\n",
    "        'postprocess': lambda gdf: add_wetland_attributes(gdf)\n",
    "    },\n",
    "    'protected_lands': {\n",
    "        'gdb_config': lambda state: {\n",
    "            'folder': PROTECTED_LANDS,\n",
    "            'subfolder': f\"PADUS4_1_State_{state}_GDB_KMZ\",\n",
    "            'gdb_name': f\"PADUS4_1_State{state}.gdb\"\n",
    "        },\n",
    "        'requires_state': True,\n",
    "        'cleanup':  lambda gdf: gdf.drop(columns=[\n",
    "                'FeatClass',\n",
    "                'Category',\n",
    "                'Own_Name',\n",
    "                'Mang_Type',\n",
    "                'Mang_Name',\n",
    "                'Des_Tp',\n",
    "                'Agg_Src',\n",
    "                'GIS_Src',\n",
    "                'Src_Date',\n",
    "                'GIS_Acres',\n",
    "                'Source_PAID',\n",
    "                'Pub_Access',\n",
    "                'Access_Src',\n",
    "                'GAP_Sts',\n",
    "                'IUCN_Cat',\n",
    "                'Date_Est',\n",
    "                'Comments',\n",
    "                'Term',\n",
    "                'Duration',\n",
    "        ], errors='ignore'),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16dc279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and save dataset\n",
    "def clean_and_save_dataset(\n",
    "    dataset='railways',\n",
    "    output_format='parquet',\n",
    "    destination_path=PARQUET_INGESTION_PATH,\n",
    "    state=None\n",
    "):\n",
    "    config = DATASET_CONFIG.get(dataset)\n",
    "    if not config:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
    "\n",
    "    if config.get('requires_state') and not state:\n",
    "        raise ValueError(f\"State must be provided for {dataset}\")\n",
    "\n",
    "    # Determine path\n",
    "    if 'path' in config:\n",
    "        full_path = os.path.join(LOCAL_DATA_FOLDER, config['path'])\n",
    "        gdf = gpd.read_file(full_path, **config.get('read_kwargs', {}))\n",
    "    else:\n",
    "        # Handle GDB datasets\n",
    "        gdb_info = config['gdb_config'](state)\n",
    "        gdb_path = os.path.join(LOCAL_DATA_FOLDER, gdb_info['folder'], gdb_info['subfolder'], gdb_info['gdb_name'])\n",
    "\n",
    "        # Find largest layer\n",
    "        largest_layer = max(\n",
    "            fiona.listlayers(gdb_path),\n",
    "            key=lambda layer: len(fiona.open(gdb_path, layer=layer))\n",
    "        )\n",
    "\n",
    "        gdf = gpd.read_file(gdb_path, layer=largest_layer)\n",
    "        print(f\"Loaded {dataset} data from layer: {largest_layer} with {len(gdf)} features\")\n",
    "\n",
    "    # Optional post-processing\n",
    "    if 'cleanup' in config:\n",
    "        gdf = config['cleanup'](gdf)\n",
    "\n",
    "    if 'postprocess' in config:\n",
    "        gdf = config['postprocess'](gdf)\n",
    "        print(f\"Postprocessed {dataset} data\")\n",
    "\n",
    "    # Check that geometries are valid\n",
    "    gdf = fix_invalid_geometries(gdf)\n",
    "\n",
    "    # Set CRS\n",
    "    gdf = gdf.to_crs(crs=geo_crs)\n",
    "    print(f\"CRS of data is {gdf.crs}\")\n",
    "\n",
    "    # Save output\n",
    "    if output_format == 'parquet':\n",
    "        filename = f\"{state}_{dataset}.parquet\" if state else f\"{dataset}.parquet\"\n",
    "        filepath = os.path.join(destination_path, filename)\n",
    "        gdf.to_parquet(filepath)\n",
    "        print(f\"{filename} data cleaned and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "160c9dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials file is valid.\n"
     ]
    }
   ],
   "source": [
    "# Upload local parquet file to GCS bucket\n",
    "# First, define constants\n",
    "BUCKET = 'geospatial-projects'\n",
    "BUCKET_FOLDER = 'infra_parcels'\n",
    "CREDENTIALS_PATH =  r\"C:\\Users\\eprashar\\AppData\\Roaming\\gcloud\\application_default_credentials.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(CREDENTIALS_PATH)\n",
    "\n",
    "# Verify credentials\n",
    "utils.check_and_authenticate(CREDENTIALS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43c579fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload locally saved parquet to GCS\n",
    "def upload_parquet_to_gcs(\n",
    "        bucket_name,\n",
    "        bucket_folder,\n",
    "        dataset,\n",
    "        local_ingestion_path = PARQUET_INGESTION_PATH,\n",
    "        state=None):\n",
    "    \"\"\"\n",
    "    Uploads a local Parquet file to a GCS bucket using config structure.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): Target GCS bucket.\n",
    "        dataset (str): One of the encumbrances in EncumbranceType\n",
    "        local_file_path (str): Local parquet file path.\n",
    "        state (str, optional): State name for datasets that include it.\n",
    "    \"\"\"\n",
    "    gcs_folder = f'{bucket_folder}/{dataset}'\n",
    "    filename = f\"{state}_{dataset}.parquet\" if state else f\"{dataset}.parquet\"\n",
    "    local_file_path = os.path.join(local_ingestion_path, filename)\n",
    "    if not os.path.exists(local_file_path):\n",
    "        raise FileNotFoundError(f\"File {local_file_path} does not exist.\")\n",
    "    destination_blob_path = f\"{gcs_folder}/{filename}\"\n",
    "\n",
    "    # GCS client upload\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_path)\n",
    "    blob.upload_from_filename(local_file_path)\n",
    "\n",
    "    print(f\"Uploaded {filename} to gs://{bucket_name}/{destination_blob_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a7ac77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File GA_wetlands.parquet already exists. Skipping.\n",
      "File GA_protected_lands.parquet already exists. Skipping.\n",
      "File MO_wetlands.parquet already exists. Skipping.\n",
      "File MO_protected_lands.parquet already exists. Skipping.\n",
      "File PA_wetlands.parquet already exists. Skipping.\n",
      "File PA_protected_lands.parquet already exists. Skipping.\n",
      "File WI_wetlands.parquet already exists. Skipping.\n",
      "File WI_protected_lands.parquet already exists. Skipping.\n",
      "File NM_wetlands.parquet already exists. Skipping.\n",
      "File NM_protected_lands.parquet already exists. Skipping.\n",
      "File IL_wetlands.parquet already exists. Skipping.\n",
      "File IL_protected_lands.parquet already exists. Skipping.\n",
      "File WA_wetlands.parquet already exists. Skipping.\n",
      "File WA_protected_lands.parquet already exists. Skipping.\n",
      "File TX_wetlands.parquet already exists. Skipping.\n",
      "File TX_protected_lands.parquet already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Run all states for protected and wetlands\n",
    "# List of states and datasets\n",
    "states = ['GA', 'MO', 'PA', 'WI', 'NM', 'IL','WA','TX']\n",
    "datasets = ['wetlands', 'protected_lands']\n",
    "\n",
    "# Iterate through states and datasets\n",
    "for state in states:\n",
    "    for dataset in datasets:\n",
    "        # Construct the filename\n",
    "        filename = f\"{state}_{dataset}.parquet\" if state else f\"{dataset}.parquet\"\n",
    "        destination_file = os.path.join(PARQUET_INGESTION_PATH, filename)\n",
    "\n",
    "       # Check if the file already exists\n",
    "        if not os.path.exists(destination_file):\n",
    "            try:\n",
    "                # Attempt to run the function\n",
    "                clean_and_save_dataset(\n",
    "                    dataset=dataset,\n",
    "                    output_format='parquet',\n",
    "                    destination_path=PARQUET_INGESTION_PATH,\n",
    "                    state=state\n",
    "                )\n",
    "                print(f\"File {filename} processed and saved.\")\n",
    "            except Exception as e:\n",
    "                # Log the error and continue\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"File {filename} already exists. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54e0e11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded roadways.parquet to gs://geospatial-projects/infra_parcels/roadways/roadways.parquet\n",
      "Uploaded railways.parquet to gs://geospatial-projects/infra_parcels/railways/railways.parquet\n",
      "Uploaded GA_protected_lands.parquet to gs://geospatial-projects/infra_parcels/protected_lands/GA_protected_lands.parquet\n",
      "Uploaded MO_protected_lands.parquet to gs://geospatial-projects/infra_parcels/protected_lands/MO_protected_lands.parquet\n",
      "Uploaded PA_protected_lands.parquet to gs://geospatial-projects/infra_parcels/protected_lands/PA_protected_lands.parquet\n",
      "Uploaded WI_protected_lands.parquet to gs://geospatial-projects/infra_parcels/protected_lands/WI_protected_lands.parquet\n",
      "Uploaded NM_protected_lands.parquet to gs://geospatial-projects/infra_parcels/protected_lands/NM_protected_lands.parquet\n",
      "Uploaded IL_protected_lands.parquet to gs://geospatial-projects/infra_parcels/protected_lands/IL_protected_lands.parquet\n",
      "Uploaded WA_protected_lands.parquet to gs://geospatial-projects/infra_parcels/protected_lands/WA_protected_lands.parquet\n",
      "Uploaded TX_protected_lands.parquet to gs://geospatial-projects/infra_parcels/protected_lands/TX_protected_lands.parquet\n",
      "Uploaded GA_wetlands.parquet to gs://geospatial-projects/infra_parcels/wetlands/GA_wetlands.parquet\n",
      "Uploaded MO_wetlands.parquet to gs://geospatial-projects/infra_parcels/wetlands/MO_wetlands.parquet\n",
      "Uploaded PA_wetlands.parquet to gs://geospatial-projects/infra_parcels/wetlands/PA_wetlands.parquet\n",
      "Uploaded WI_wetlands.parquet to gs://geospatial-projects/infra_parcels/wetlands/WI_wetlands.parquet\n",
      "Uploaded NM_wetlands.parquet to gs://geospatial-projects/infra_parcels/wetlands/NM_wetlands.parquet\n",
      "Uploaded IL_wetlands.parquet to gs://geospatial-projects/infra_parcels/wetlands/IL_wetlands.parquet\n",
      "Uploaded WA_wetlands.parquet to gs://geospatial-projects/infra_parcels/wetlands/WA_wetlands.parquet\n",
      "Uploaded TX_wetlands.parquet to gs://geospatial-projects/infra_parcels/wetlands/TX_wetlands.parquet\n",
      "Uploaded transmission_lines.parquet to gs://geospatial-projects/infra_parcels/transmission_lines/transmission_lines.parquet\n"
     ]
    }
   ],
   "source": [
    "# List of states and datasets\n",
    "states = ['GA', 'MO', 'PA', 'WI', 'NM', 'IL', 'WA', 'TX']\n",
    "datasets = ENCUMBRANCES\n",
    "\n",
    "for dataset in datasets:\n",
    "    if dataset in ['wetlands', 'protected_lands']:\n",
    "        for state in states:\n",
    "            try:\n",
    "                upload_parquet_to_gcs(\n",
    "                    bucket_name=BUCKET,\n",
    "                    bucket_folder=BUCKET_FOLDER, \n",
    "                    dataset=dataset, \n",
    "                    local_ingestion_path=PARQUET_INGESTION_PATH, \n",
    "                    state=state\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {dataset} for {state}: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            upload_parquet_to_gcs(\n",
    "                bucket_name=BUCKET,\n",
    "                bucket_folder=BUCKET_FOLDER,\n",
    "                dataset=dataset, \n",
    "                local_ingestion_path=PARQUET_INGESTION_PATH\n",
    "                # No state needed\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {dataset}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e573d998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in railway data are 94619\n",
      "   OBJECTID      ID           TYPE         STATUS NAICS_CODE  \\\n",
      "0         1  100000       OVERHEAD     IN SERVICE     221121   \n",
      "1         2  100001   AC; OVERHEAD     IN SERVICE     221121   \n",
      "2         3  100002   AC; OVERHEAD     IN SERVICE     221121   \n",
      "3         4  100003  NOT AVAILABLE  NOT AVAILABLE     221121   \n",
      "4         5  100004       OVERHEAD     IN SERVICE     221121   \n",
      "\n",
      "                                     NAICS_DESC  \\\n",
      "0  ELECTRIC BULK POWER TRANSMISSION AND CONTROL   \n",
      "1  ELECTRIC BULK POWER TRANSMISSION AND CONTROL   \n",
      "2  ELECTRIC BULK POWER TRANSMISSION AND CONTROL   \n",
      "3  ELECTRIC BULK POWER TRANSMISSION AND CONTROL   \n",
      "4  ELECTRIC BULK POWER TRANSMISSION AND CONTROL   \n",
      "\n",
      "                                              SOURCE  SOURCEDATE  VAL_METHOD  \\\n",
      "0                             IMAGERY, OpenStreetMap  2015-07-07     IMAGERY   \n",
      "1                                   IMAGERY, EIA 861  2014-05-02     IMAGERY   \n",
      "2  IMAGERY, EIA 861, https://www.tva.gov/file_sou...  2014-04-24     IMAGERY   \n",
      "3                                            IMAGERY  2016-12-09  UNVERIFIED   \n",
      "4                             IMAGERY, OpenStreetMap  2014-08-21     IMAGERY   \n",
      "\n",
      "     VAL_DATE                        OWNER   VOLTAGE     VOLT_CLASS INFERRED  \\\n",
      "0  2016-02-17                NOT AVAILABLE -999999.0  NOT AVAILABLE        N   \n",
      "1  2020-04-16             ALABAMA POWER CO     115.0        100-161        Y   \n",
      "2  2019-03-11   TENNESSEE VALLEY AUTHORITY     161.0        100-161        Y   \n",
      "3  2016-12-09  PPL ELECTRIC UTILITIES CORP -999999.0  NOT AVAILABLE        N   \n",
      "4  2014-08-21                NOT AVAILABLE -999999.0      UNDER 100        Y   \n",
      "\n",
      "           SUB_1      SUB_2  Shape__Len  \\\n",
      "0  UNKNOWN128553  TAP139917     2250.17   \n",
      "1  UNKNOWN109715  TAP142776    19187.28   \n",
      "2  UNKNOWN109571  TAP144762     5619.80   \n",
      "3  UNKNOWN125207  TAP147512      481.78   \n",
      "4  UNKNOWN115971  TAP142027       21.15   \n",
      "\n",
      "                                            geometry  \n",
      "0  LINESTRING (-84.41732 42.73387, -84.41732 42.7...  \n",
      "1  LINESTRING (-87.67469 30.39959, -87.67272 30.3...  \n",
      "2  LINESTRING (-88.44996 33.58228, -88.44994 33.5...  \n",
      "3  LINESTRING (-76.81857 40.90397, -76.81726 40.9...  \n",
      "4  LINESTRING (-93.10151 31.72680, -93.10151 31.7...  \n"
     ]
    }
   ],
   "source": [
    "# REMOVE THIS LATER - TESTING CHUNK\n",
    "# Load railways parquet file\n",
    "transmission = gpd.read_parquet(os.path.join(PARQUET_INGESTION_PATH, 'transmission_lines.parquet'))\n",
    "print(f'Total rows in railway data are {transmission.shape[0]}')\n",
    "#print(f'Rows where STCNTYFIPS is null are {railways[railways['STCNTYFIPS'].isnull()].shape[0]}')\n",
    "#print(f'Rows where KM value is zero are {railways[railways['KM'] == 0].shape[0]}')\n",
    "print(transmission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93ef96",
   "metadata": {},
   "source": [
    "#### 2.1 Save county level data for encumbrances in parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c9e25f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials file is valid.\n"
     ]
    }
   ],
   "source": [
    "# Upload local parquet file to GCS bucket\n",
    "# First, define constants\n",
    "GIS_PROJECT = 'clgx-gis-app-dev-06e3'\n",
    "POC_DATASET = 'encumbered_parcels'\n",
    "CREDENTIALS_PATH =  r\"C:\\Users\\eprashar\\AppData\\Roaming\\gcloud\\application_default_credentials.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(CREDENTIALS_PATH)\n",
    "\n",
    "# Verify credentials\n",
    "utils.check_and_authenticate(CREDENTIALS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bc28151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload county data in GCS bucket after converting CRS to EPSG:4326\n",
    "def convert_upload_parquet_to_gcs_and_bigquery(\n",
    "    source_folder,\n",
    "    output_name,\n",
    "    bucket_name,\n",
    "    gcs_blob,\n",
    "    project_id,\n",
    "    dataset_id,\n",
    "    table_name\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts a Parquet file with EPSG:4269 geometries to EPSG:4326,\n",
    "    uploads it to GCS, then loads it into BigQuery.\n",
    "\n",
    "    Args:\n",
    "    \"\"\"\n",
    "    # Load and convert CRS\n",
    "    input_path = os.path.join(LOCAL_DATA_FOLDER, source_folder)\n",
    "    gdf = gpd.read_file(input_path)\n",
    "    print(f\"Loaded {len(gdf)} features from {input_path}\")\n",
    "\n",
    "    if gdf.crs is None or gdf.crs.to_epsg() != geo_crs:\n",
    "        gdf = gdf.to_crs(geo_crs)\n",
    "        print(\"CRS converted to EPSG:4326\")\n",
    "    else:\n",
    "        print(\"CRS already in EPSG:4326\")\n",
    "\n",
    "    # Save converted file locally\n",
    "    output_path = os.path.join(PARQUET_INGESTION_PATH, f'{output_name}')\n",
    "    gdf.to_parquet(output_path)\n",
    "    print(f\"Saved converted file to {output_path}\")\n",
    "\n",
    "    # Upload to GCS\n",
    "    gcs_blob_path = f'{gcs_blob}/{output_name}'\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(gcs_blob_path)\n",
    "    blob.upload_from_filename(output_path)\n",
    "    print(f\"Uploaded to GCS: gs://{bucket_name}/{gcs_blob_path}\")\n",
    "\n",
    "    # Load into BigQuery\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.PARQUET,\n",
    "        autodetect=True,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    )\n",
    "\n",
    "    uri = f\"gs://{bucket_name}/{gcs_blob_path}\"\n",
    "    load_job = client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "    load_job.result()  # Wait for job to complete\n",
    "\n",
    "    print(f\"Loaded data into BigQuery table: {table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52edac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3235 features from C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\counties\\tl_2024_us_county\\tl_2024_us_county.shp\n",
      "CRS converted to EPSG:4326\n",
      "Saved converted file to C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\\county_bounds.parquet\n",
      "Uploaded to GCS: gs://geospatial-projects/infra_parcels/county_bounds/county_bounds.parquet\n",
      "Loaded data into BigQuery table: clgx-gis-app-dev-06e3.encumbered_parcels.county_boundaries\n"
     ]
    }
   ],
   "source": [
    "# Executing the county data upload\n",
    "convert_upload_parquet_to_gcs_and_bigquery(\n",
    "    source_folder=COUNTY_DATA,\n",
    "    output_name='county_bounds.parquet',\n",
    "    bucket_name=BUCKET,\n",
    "    gcs_blob= f'{BUCKET_FOLDER}/county_bounds',\n",
    "    project_id=GIS_PROJECT,\n",
    "    dataset_id=POC_DATASET,\n",
    "    table_name=\"county_boundaries\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8052db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load county boundary\n",
    "def get_county_boundary(fips_code):\n",
    "    \"\"\"\n",
    "    Load county boundary from shapefile and filter by FIPS code.\n",
    "    \"\"\"\n",
    "    # Read the shapefile\n",
    "    gdf_county = gpd.read_file(os.path.join(LOCAL_DATA_FOLDER, COUNTY_DATA))\n",
    "    \n",
    "    # Filter by FIPS code\n",
    "    gdf_county = gdf_county[gdf_county['GEOID'] == fips_code]\n",
    "    \n",
    "    # Convert to EPSG:4326\n",
    "    gdf_county = gdf_county.to_crs(geo_crs)\n",
    "    \n",
    "    return gdf_county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "938b2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter encumbrance data for county\n",
    "def filter_gdf_using_boundary(gdf_encumbrance, county_boundary):\n",
    "    \"\"\"\n",
    "    Load wetland attributes from a geodatabase and join them to the wetlands GeoDataFrame\n",
    "    using the 'ATTRIBUTE' field as a key.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: \n",
    "    \"\"\"\n",
    "\n",
    "    # Convert both dataframes to the same projection system\n",
    "    gdf_encumbrance = gdf_encumbrance.to_crs(county_boundary.crs)\n",
    "\n",
    "    # Perform a spatial join\n",
    "    filtered_encumbrance = gpd.sjoin(gdf_encumbrance, county_boundary, predicate='within')\n",
    "\n",
    "    # Drop unnecessary columns from county database\n",
    "    filtered_encumbrance.drop(columns=[\n",
    "        'index_right',\n",
    "        'STATEFP',\n",
    "        'COUNTYFP',\n",
    "        'COUNTYNS',\n",
    "        'GEOID',\n",
    "        'GEOIDFQ',\n",
    "        'LSAD',\n",
    "        'CLASSFP',\n",
    "        'MTFCC',\n",
    "        'CSAFP',\n",
    "        'CBSAFP',\n",
    "        'METDIVFP',\n",
    "        'FUNCSTAT',\n",
    "        'ALAND',\n",
    "        'AWATER',\n",
    "        'INTPTLAT',\n",
    "        'INTPTLON'], inplace=True)\n",
    "\n",
    "    # Convert epsg for filtered gdf to 4326\n",
    "    filtered_encumbrance = filtered_encumbrance.set_geometry('geometry').to_crs(geo_crs)\n",
    "    print(f'CRS of the filtered dataframe is {filtered_encumbrance.crs}')\n",
    "    return filtered_encumbrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13692134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for encumbrance type\n",
    "def load_encumbrance_for_county(\n",
    "        encumbrance_type: EncumbranceType,\n",
    "        fips,\n",
    "        state=None) -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    Load encumbrance data from <source> and return as GeoDataFrame.\n",
    "    '''\n",
    "    # Flag error if chosen encumbrance is not defined\n",
    "    if encumbrance_type not in ENCUMBRANCES:\n",
    "        raise ValueError(\n",
    "            f\"Invalid encumbrance type '{encumbrance_type}'. \"\n",
    "            f\"Valid options are: {', '.join(ENCUMBRANCES)}.\"\n",
    "        )\n",
    "    # Obtain county boundary\n",
    "    county_boundary = get_county_boundary(fips)\n",
    "\n",
    "    # Read the parquet file for the encumbrance type\n",
    "    try:\n",
    "        file_path = os.path.join(PARQUET_INGESTION_PATH, f'{state}_{encumbrance_type}.parquet' if state else f'{encumbrance_type}.parquet')\n",
    "        gdf = gpd.read_parquet(file_path)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"File {file_path} not found. Error: {e}\")\n",
    "\n",
    "    # Use spatial join to get encumbrance data for county\n",
    "    county_gdf = filter_gdf_using_boundary(gdf, county_boundary)\n",
    "\n",
    "    # save county_gdf as parquet file\n",
    "    print(f\"{encumbrance_type} data loaded for FIPS code: {fips}\")\n",
    "    \n",
    "    # Convert to EPSG:4326\n",
    "    county_gdf = county_gdf.to_crs(geo_crs)\n",
    "    county_gdf.to_parquet(os.path.join(PARQUET_INGESTION_PATH,f'{fips}_{encumbrance_type}.parquet'))\n",
    "    print(f'{encumbrance_type} parquet created for {fips}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6cd64102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 17031_roadways.parquet already exists. Skipping.\n",
      "File 17031_railways.parquet already exists. Skipping.\n",
      "File 17031_protected_lands.parquet already exists. Skipping.\n",
      "File 17031_wetlands.parquet already exists. Skipping.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 17031\n",
      "transmission_lines parquet created for 17031!\n",
      "File 17031_transmission_lines.parquet processed and saved.\n",
      "File 13121_roadways.parquet already exists. Skipping.\n",
      "File 13121_railways.parquet already exists. Skipping.\n",
      "File 13121_protected_lands.parquet already exists. Skipping.\n",
      "File 13121_wetlands.parquet already exists. Skipping.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 13121\n",
      "transmission_lines parquet created for 13121!\n",
      "File 13121_transmission_lines.parquet processed and saved.\n",
      "File 53033_roadways.parquet already exists. Skipping.\n",
      "File 53033_railways.parquet already exists. Skipping.\n",
      "File 53033_protected_lands.parquet already exists. Skipping.\n",
      "File 53033_wetlands.parquet already exists. Skipping.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 53033\n",
      "transmission_lines parquet created for 53033!\n",
      "File 53033_transmission_lines.parquet processed and saved.\n",
      "File 48491_roadways.parquet already exists. Skipping.\n",
      "File 48491_railways.parquet already exists. Skipping.\n",
      "File 48491_protected_lands.parquet already exists. Skipping.\n",
      "File 48491_wetlands.parquet already exists. Skipping.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 48491\n",
      "transmission_lines parquet created for 48491!\n",
      "File 48491_transmission_lines.parquet processed and saved.\n",
      "File 29181_roadways.parquet already exists. Skipping.\n",
      "File 29181_railways.parquet already exists. Skipping.\n",
      "File 29181_protected_lands.parquet already exists. Skipping.\n",
      "File 29181_wetlands.parquet already exists. Skipping.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 29181\n",
      "transmission_lines parquet created for 29181!\n",
      "File 29181_transmission_lines.parquet processed and saved.\n",
      "File 42011_roadways.parquet already exists. Skipping.\n",
      "File 42011_railways.parquet already exists. Skipping.\n",
      "File 42011_protected_lands.parquet already exists. Skipping.\n",
      "File 42011_wetlands.parquet already exists. Skipping.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 42011\n",
      "transmission_lines parquet created for 42011!\n",
      "File 42011_transmission_lines.parquet processed and saved.\n",
      "File 55107_roadways.parquet already exists. Skipping.\n",
      "File 55107_railways.parquet already exists. Skipping.\n",
      "File 55107_protected_lands.parquet already exists. Skipping.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "wetlands data loaded for FIPS code: 55107\n",
      "wetlands parquet created for 55107!\n",
      "File 55107_wetlands.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 55107\n",
      "transmission_lines parquet created for 55107!\n",
      "File 55107_transmission_lines.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "roadways data loaded for FIPS code: 35051\n",
      "roadways parquet created for 35051!\n",
      "File 35051_roadways.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "railways data loaded for FIPS code: 35051\n",
      "railways parquet created for 35051!\n",
      "File 35051_railways.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "protected_lands data loaded for FIPS code: 35051\n",
      "protected_lands parquet created for 35051!\n",
      "File 35051_protected_lands.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "wetlands data loaded for FIPS code: 35051\n",
      "wetlands parquet created for 35051!\n",
      "File 35051_wetlands.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 35051\n",
      "transmission_lines parquet created for 35051!\n",
      "File 35051_transmission_lines.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "roadways data loaded for FIPS code: 17127\n",
      "roadways parquet created for 17127!\n",
      "File 17127_roadways.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "railways data loaded for FIPS code: 17127\n",
      "railways parquet created for 17127!\n",
      "File 17127_railways.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "protected_lands data loaded for FIPS code: 17127\n",
      "protected_lands parquet created for 17127!\n",
      "File 17127_protected_lands.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "wetlands data loaded for FIPS code: 17127\n",
      "wetlands parquet created for 17127!\n",
      "File 17127_wetlands.parquet processed and saved.\n",
      "CRS of the filtered dataframe is {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"GeographicCRS\", \"name\": \"WGS 84\", \"datum_ensemble\": {\"name\": \"World Geodetic System 1984 ensemble\", \"members\": [{\"name\": \"World Geodetic System 1984 (Transit)\"}, {\"name\": \"World Geodetic System 1984 (G730)\"}, {\"name\": \"World Geodetic System 1984 (G873)\"}, {\"name\": \"World Geodetic System 1984 (G1150)\"}, {\"name\": \"World Geodetic System 1984 (G1674)\"}, {\"name\": \"World Geodetic System 1984 (G1762)\"}, {\"name\": \"World Geodetic System 1984 (G2139)\"}], \"ellipsoid\": {\"name\": \"WGS 84\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257223563}, \"accuracy\": \"2.0\", \"id\": {\"authority\": \"EPSG\", \"code\": 6326}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"scope\": \"Horizontal component of 3D system.\", \"area\": \"World.\", \"bbox\": {\"south_latitude\": -90, \"west_longitude\": -180, \"north_latitude\": 90, \"east_longitude\": 180}, \"id\": {\"authority\": \"EPSG\", \"code\": 4326}}\n",
      "transmission_lines data loaded for FIPS code: 17127\n",
      "transmission_lines parquet created for 17127!\n",
      "File 17127_transmission_lines.parquet processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Mapping of county FIPS to state abbreviations\n",
    "FIPS_TO_STATE = {\n",
    "    '17031': 'IL',  # Cook County, IL\n",
    "    '13121': 'GA',  # Fulton County, GA\n",
    "    '53033': 'WA',  # King County, WA\n",
    "    '48491': 'TX',  # Williamson County, TX\n",
    "    '29181': 'MO',  # Warren County, MO\n",
    "    '42011': 'PA',  # Berks County, PA\n",
    "    '55107': 'WI',  # Rusk County, WI\n",
    "    '35051': 'NM',  # Sierra County, NM\n",
    "    '17127': 'IL',  # Massac County, IL\n",
    "}\n",
    "\n",
    "poc_fips = POC_FINALIZED_COUNTIES\n",
    "encumbrances = ENCUMBRANCES\n",
    "\n",
    "# Encumbrances that require state information\n",
    "STATE_REQUIRED_TYPES = {'wetlands', 'protected_lands'}\n",
    "\n",
    "for fips in poc_fips:\n",
    "    for encumbrance in encumbrances:\n",
    "        filename = f\"{fips}_{encumbrance}.parquet\"\n",
    "        destination_file = os.path.join(PARQUET_INGESTION_PATH, filename)\n",
    "\n",
    "        if not os.path.exists(destination_file):\n",
    "            try:\n",
    "                state = FIPS_TO_STATE.get(fips) if encumbrance in STATE_REQUIRED_TYPES else None\n",
    "\n",
    "                load_encumbrance_for_county(\n",
    "                    encumbrance_type=encumbrance,\n",
    "                    fips=fips,\n",
    "                    state=state\n",
    "                )\n",
    "                print(f\"File {filename} processed and saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"File {filename} already exists. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003bbf94",
   "metadata": {},
   "source": [
    "#### 3. [Only for POC development in python]: Parcel data for POC counties from BigQ in parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce6d8c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials file is valid.\n"
     ]
    }
   ],
   "source": [
    "# Define constants here\n",
    "PROJECT = 'clgx-gis-app-dev-06e3'\n",
    "DATASET = 'property'\n",
    "POC_DATASET = 'encumbered_parcels'\n",
    "POC_TABLE = 'base_parcels_poc_counties_0419'\n",
    "CREDENTIALS_PATH =  r\"C:\\Users\\eprashar\\AppData\\Roaming\\gcloud\\application_default_credentials.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(CREDENTIALS_PATH)\n",
    "\n",
    "# Credentials verification\n",
    "utils.check_and_authenticate(CREDENTIALS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "253f303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get parcel data for the defined county\n",
    "def fetch_and_save_county_parcels(fips_code: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load parcel data from BigQuery and filter by FIPS code.\n",
    "    \"\"\"\n",
    "    # Define the SQL query to filter by FIPS code\n",
    "    # TO-DO: Create a table in BQ with processed data\n",
    "    query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM `{PROJECT}.{POC_DATASET}.{POC_TABLE}`\n",
    "        WHERE fips_code = '{fips_code}'\n",
    "    \"\"\"\n",
    "    # Read the data into a GeoDataFrame\n",
    "    gdf_parcel = utils.read_bigquery_to_gdf(project=PROJECT, dataset=POC_DATASET, table=POC_TABLE, query=query, output='gpd', geometry_col='geometry')\n",
    "    \n",
    "    # Convert to EPSG:4326\n",
    "    gdf_parcel = gdf_parcel.to_crs(geo_crs)\n",
    "    print(f'CRS of the parcel dataframe is {gdf_parcel.crs}')\n",
    "    gdf_parcel.to_parquet(os.path.join(PARQUET_INGESTION_PATH,f'{fips_code}_parcels.parquet'))\n",
    "    print(f'parcel parquet created for {fips_code}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73270b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save parcel parquets for all POC counties\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m county_fips \u001b[38;5;129;01min\u001b[39;00m POC_FINALIZED_COUNTIES:\n\u001b[1;32m----> 3\u001b[0m     fetch_and_save_county_parcels(fips_code\u001b[38;5;241m=\u001b[39mcounty_fips)\n",
      "Cell \u001b[1;32mIn[38], line 14\u001b[0m, in \u001b[0;36mfetch_and_save_county_parcels\u001b[1;34m(fips_code)\u001b[0m\n\u001b[0;32m      8\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m    SELECT * \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m    FROM `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPOC_DATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPOC_TABLE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m    WHERE fips_code = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfips_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Read the data into a GeoDataFrame\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m gdf_parcel \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mread_bigquery_to_gdf(project\u001b[38;5;241m=\u001b[39mPROJECT, dataset\u001b[38;5;241m=\u001b[39mPOC_DATASET, table\u001b[38;5;241m=\u001b[39mPOC_TABLE, query\u001b[38;5;241m=\u001b[39mquery, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpd\u001b[39m\u001b[38;5;124m'\u001b[39m, geometry_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Convert to EPSG:4326\u001b[39;00m\n\u001b[0;32m     17\u001b[0m gdf_parcel \u001b[38;5;241m=\u001b[39m gdf_parcel\u001b[38;5;241m.\u001b[39mto_crs(geo_crs)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\feb_25_encumbered_parcels\\enc-parcels\\poc_scripts\\utils.py:85\u001b[0m, in \u001b[0;36mread_bigquery_to_gdf\u001b[1;34m(project, dataset, table, query, output, geometry_col)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query:\n\u001b[0;32m     84\u001b[0m     query_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(query)\n\u001b[1;32m---> 85\u001b[0m     df \u001b[38;5;241m=\u001b[39m query_job\u001b[38;5;241m.\u001b[39mto_dataframe()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     table_ref \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mdataset(dataset)\u001b[38;5;241m.\u001b[39mtable(table)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:2057\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[1;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dataframe\u001b[39m(\n\u001b[0;32m   1828\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1829\u001b[0m     bqstorage_client: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     ] \u001b[38;5;241m=\u001b[39m DefaultPandasDTypes\u001b[38;5;241m.\u001b[39mRANGE_TIMESTAMP_DTYPE,\n\u001b[0;32m   1850\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[0;32m   1852\u001b[0m \n\u001b[0;32m   1853\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;124;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[0;32m   2056\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2057\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m wait_for_query(\u001b[38;5;28mself\u001b[39m, progress_bar_type, max_results\u001b[38;5;241m=\u001b[39mmax_results)\n\u001b[0;32m   2058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result\u001b[38;5;241m.\u001b[39mto_dataframe(\n\u001b[0;32m   2059\u001b[0m         bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[0;32m   2060\u001b[0m         dtypes\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m         range_timestamp_dtype\u001b[38;5;241m=\u001b[39mrange_timestamp_dtype,\n\u001b[0;32m   2075\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\bigquery\\_tqdm_helpers.py:107\u001b[0m, in \u001b[0;36mwait_for_query\u001b[1;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[0;32m    103\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m get_progress_bar(\n\u001b[0;32m    104\u001b[0m     progress_bar_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery is running\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_total, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query_job\u001b[38;5;241m.\u001b[39mresult(max_results\u001b[38;5;241m=\u001b[39mmax_results)\n\u001b[0;32m    109\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1681\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[1;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[1;32m-> 1681\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_job_done():\n\u001b[0;32m   1682\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1684\u001b[0m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1650\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.is_job_done\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;66;03m# Call jobs.getQueryResults with max results set to 0 just to\u001b[39;00m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;66;03m# wait for the query to finish. Unlike most methods,\u001b[39;00m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;66;03m# jobs.getQueryResults hangs as long as it can to ensure we\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;66;03m# know when the query has finished as soon as possible.\u001b[39;00m\n\u001b[1;32m-> 1650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reload_query_results(retry\u001b[38;5;241m=\u001b[39mretry, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreload_query_results_kwargs)\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;66;03m# Even if the query is finished now according to\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;66;03m# jobs.getQueryResults, we'll want to reload the job status if\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;66;03m# it's not already DONE.\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1448\u001b[0m, in \u001b[0;36mQueryJob._reload_query_results\u001b[1;34m(self, retry, timeout, page_size)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transport_timeout, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m   1446\u001b[0m         transport_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_get_query_results(\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id,\n\u001b[0;32m   1450\u001b[0m     retry,\n\u001b[0;32m   1451\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject,\n\u001b[0;32m   1452\u001b[0m     timeout_ms\u001b[38;5;241m=\u001b[39mtimeout_ms,\n\u001b[0;32m   1453\u001b[0m     location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation,\n\u001b[0;32m   1454\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtransport_timeout,\n\u001b[0;32m   1455\u001b[0m     page_size\u001b[38;5;241m=\u001b[39mpage_size,\n\u001b[0;32m   1456\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\bigquery\\client.py:2034\u001b[0m, in \u001b[0;36mClient._get_query_results\u001b[1;34m(self, job_id, retry, project, timeout_ms, location, timeout, page_size)\u001b[0m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;66;03m# This call is typically made in a polling loop that checks whether the\u001b[39;00m\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;66;03m# job is complete (from QueryJob.done(), called ultimately from\u001b[39;00m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;66;03m# QueryJob.result()). So we don't need to poll here.\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[1;32m-> 2034\u001b[0m resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_api(\n\u001b[0;32m   2035\u001b[0m     retry,\n\u001b[0;32m   2036\u001b[0m     span_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigQuery.getQueryResults\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2037\u001b[0m     span_attributes\u001b[38;5;241m=\u001b[39mspan_attributes,\n\u001b[0;32m   2038\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2039\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2040\u001b[0m     query_params\u001b[38;5;241m=\u001b[39mextra_params,\n\u001b[0;32m   2041\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   2042\u001b[0m )\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _QueryResults\u001b[38;5;241m.\u001b[39mfrom_api_repr(resource)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\bigquery\\client.py:843\u001b[0m, in \u001b[0;36mClient._call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[0;32m    841\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[0;32m    842\u001b[0m     ):\n\u001b[1;32m--> 843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m call()\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\_http\\__init__.py:482\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    479\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(data)\n\u001b[0;32m    480\u001b[0m     content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    485\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    486\u001b[0m     content_type\u001b[38;5;241m=\u001b[39mcontent_type,\n\u001b[0;32m    487\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    488\u001b[0m     target_object\u001b[38;5;241m=\u001b[39m_target_object,\n\u001b[0;32m    489\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\_http\\__init__.py:341\u001b[0m, in \u001b[0;36mJSONConnection._make_request\u001b[1;34m(self, method, url, data, content_type, headers, target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    338\u001b[0m     headers[CLIENT_INFO_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\n\u001b[0;32m    339\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_request(\n\u001b[0;32m    342\u001b[0m     method, url, headers, data, target_object, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    343\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\_http\\__init__.py:379\u001b[0m, in \u001b[0;36mJSONConnection._do_request\u001b[1;34m(self, method, url, headers, data, target_object, timeout)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_request\u001b[39m(\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, url, headers, data, target_object, timeout\u001b[38;5;241m=\u001b[39m_DEFAULT_TIMEOUT\n\u001b[0;32m    347\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Low-level helper:  perform the actual API request over HTTP.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m    Allows batch context managers to override and defer a request.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    :returns: The HTTP response.\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    380\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, method\u001b[38;5;241m=\u001b[39mmethod, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    381\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\auth\\transport\\requests.py:537\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[1;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(AuthorizedSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    538\u001b[0m         method,\n\u001b[0;32m    539\u001b[0m         url,\n\u001b[0;32m    540\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    541\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[0;32m    542\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    545\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    547\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save parcel parquets for all POC counties\n",
    "for county_fips in POC_FINALIZED_COUNTIES:\n",
    "    fetch_and_save_county_parcels(fips_code=county_fips)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
