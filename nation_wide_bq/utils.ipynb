{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bc00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import time\n",
    "from google.cloud import storage\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery_datatransfer\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e2323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to check authentication key\n",
    "# Function to check google authentication token and re-generate if it is expired/doesn't exist\n",
    "def check_and_authenticate(json_path):\n",
    "    '''\n",
    "    Function to check google authentication token and re-generate if it is expired/doesn't exist\n",
    "    '''\n",
    "    try:\n",
    "        if not os.path.exists(json_path):\n",
    "            raise FileNotFoundError(\"Credentials file not found\")\n",
    "        # Get modification time of the file\n",
    "        file_mod_time = datetime.fromtimestamp(os.path.getmtime(json_path))\n",
    "        current_time = datetime.now()\n",
    "\n",
    "        # Check if the file is older than 24 hours\n",
    "        if current_time - file_mod_time > timedelta(hours=24):\n",
    "            print(\"Credentials file is older than 24 hours. Re-authenticating...\")\n",
    "\n",
    "            # Re-authenticate\n",
    "            try:\n",
    "                print(f\"Trying reauthentication on gcloud server using shell command...\")\n",
    "                subprocess.run(\"start cmd /c gcloud auth application-default login\", shell=True, check=True)\n",
    "                print('Login window opened...please complete authentication')\n",
    "                \n",
    "                # Poll for file modification\n",
    "                print(\"Waiting for credentials file to update...\")\n",
    "                max_wait = 300  # seconds\n",
    "                check_interval = 2  # seconds\n",
    "                start_time = datetime.now()\n",
    "\n",
    "                while (datetime.now() - start_time).total_seconds() < max_wait:\n",
    "                    new_mod_time = datetime.fromtimestamp(os.path.getmtime(json_path))\n",
    "                    if new_mod_time > file_mod_time:\n",
    "                        print(\"Authentication confirmed! Credentials file updated.\")\n",
    "                        break\n",
    "                    time.sleep(check_interval)\n",
    "                else:\n",
    "                    print(\"Timed out waiting for credentials file update.\")\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error during re-authentication: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f'Authentication failed because of {e}')\n",
    "        else:\n",
    "            print(\"Credentials file is valid.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72d9ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials file is older than 24 hours. Re-authenticating...\n",
      "Trying reauthentication on gcloud server using shell command...\n",
      "Login window opened...please complete authentication\n",
      "Waiting for credentials file to update...\n",
      "Authentication confirmed! Credentials file updated.\n"
     ]
    }
   ],
   "source": [
    "# Uploading to GCS\n",
    "# First, validate the authentication token\n",
    "CREDENTIALS_PATH =  r\"C:\\Users\\eprashar\\AppData\\Roaming\\gcloud\\application_default_credentials.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(CREDENTIALS_PATH)\n",
    "\n",
    "# Check and authenticate\n",
    "check_and_authenticate(CREDENTIALS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dfcd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not using a service account, initialize the client like this:\n",
    "SOURCE_PROJECT_ID = \"clgx-gis-app-uat-a0e0\"\n",
    "SOURCE_DATASET_ID = \"proximity_parcels\"\n",
    "\n",
    "DESTINATION_PROJECT_ID = \"clgx-gis-app-prd-364d\"\n",
    "DESTINATION_DATASET_ID = \"proximity_parcels\"\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID)\n",
    "\n",
    "# ==============================================================================\n",
    "# Function to copy all tables from source dataset to destination dataset\n",
    "# ==============================================================================\n",
    "def copy_all_tables(\n",
    "        source_project, \n",
    "        source_dataset, \n",
    "        dest_project, \n",
    "        dest_dataset,\n",
    "        tables=None,\n",
    "        overwrite=False\n",
    "        ):\n",
    "    \"\"\"\n",
    "   Copies tables from a source dataset to a destination dataset.\n",
    "    - If 'table_list' is provided, it copies only those tables.\n",
    "    - If 'table_list' is None, it copies all tables from the source.\n",
    "    - It will automatically skip any materialized views.\n",
    "    - If 'overwrite' is False, it will skip tables that already exist in the destination.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Table Copy ---\")\n",
    "    print(f\"From: {source_project}.{source_dataset}\")\n",
    "    print(f\"To:   {dest_project}.{dest_dataset}\")\n",
    "\n",
    "    source_dataset_ref = f\"{source_project}.{source_dataset}\"\n",
    "    \n",
    "    try:\n",
    "        # Determine which tables to process\n",
    "        tables_to_process = []\n",
    "        if tables:\n",
    "            print(f\"Processing a provided list of {len(tables)} tables.\")\n",
    "            tables_to_process = tables\n",
    "        else:\n",
    "            print(\"No table list provided. Fetching all tables from source dataset.\")\n",
    "            all_items = client.list_tables(source_dataset_ref)\n",
    "            tables_to_process = [item.table_id for item in all_items]\n",
    "            print(f\"Found {len(tables_to_process)} items in source dataset.\")\n",
    "\n",
    "        for table_id in tables_to_process:\n",
    "            source_table_ref_str = f\"{source_project}.{source_dataset}.{table_id}\"\n",
    "        \n",
    "            # Get the full table object to check its type\n",
    "            try:\n",
    "                table_obj = client.get_table(source_table_ref_str)\n",
    "            except NotFound:\n",
    "                print(f\"  -> WARNING: Table '{table_id}' not found in source dataset. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Check the table_type\n",
    "            if table_obj.table_type == \"MATERIALIZED_VIEW\":\n",
    "                print(f\"  -> Skipping: {table_id} (Type: MATERIALIZED_VIEW)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  -> Copying table: {table_id}...\")\n",
    "            dest_table_ref_str = f\"{dest_project}.{dest_dataset}.{table_id}\"\n",
    "\n",
    "            # Additional check for overwrite functionality\n",
    "            if not overwrite:\n",
    "                try:\n",
    "                    client.get_table(dest_table_ref_str)\n",
    "                    # If get_table succeeds, the table exists.\n",
    "                    print(f\"  -> Skipping: Destination table '{dest_table_ref_str}' already exists and overwrite is False.\")\n",
    "                    continue\n",
    "                except NotFound:\n",
    "                    # Table doesn't exist, so we can proceed with the copy.\n",
    "                    pass\n",
    "\n",
    "            # Configure and start the copy job\n",
    "            job_config = bigquery.CopyJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "            copy_job = client.copy_table(\n",
    "                source_table_ref_str,\n",
    "                dest_table_ref_str,\n",
    "                job_config=job_config,\n",
    "            )\n",
    "            copy_job.result()  # Wait for the job to complete\n",
    "            print(f\"      -> SUCCESS: Copied to {dest_table_ref_str}\")\n",
    "\n",
    "    except NotFound:\n",
    "        print(f\"ERROR: Source dataset '{source_dataset_ref}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during table copy: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56a8ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Table Copy ---\n",
      "From: clgx-gis-app-uat-a0e0.proximity_parcels\n",
      "To:   clgx-gis-app-prd-364d.proximity_parcels\n",
      "Processing a provided list of 5 tables.\n",
      "  -> Copying table: roadways...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.roadways' already exists and overwrite is False.\n",
      "  -> Copying table: railways...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.railways' already exists and overwrite is False.\n",
      "  -> Copying table: transmission_lines...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.transmission_lines' already exists and overwrite is False.\n",
      "  -> Copying table: protected_lands_national...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.protected_lands_national' already exists and overwrite is False.\n",
      "  -> Copying table: wetlands...\n",
      "  -> Skipping: Destination table 'clgx-gis-app-prd-364d.proximity_parcels.wetlands' already exists and overwrite is False.\n"
     ]
    }
   ],
   "source": [
    "# Copy tables from GIS UAT to PROD\n",
    "tables_to_copy = [\n",
    "    'roadways',\n",
    "    'railways',\n",
    "    'transmission_lines',\n",
    "    'protected_lands_national',\n",
    "    'wetlands'\n",
    "    ]\n",
    "\n",
    "# Run copy all tables\n",
    "copy_all_tables(\n",
    "    SOURCE_PROJECT_ID, \n",
    "    SOURCE_DATASET_ID, \n",
    "    DESTINATION_PROJECT_ID, \n",
    "    DESTINATION_DATASET_ID,\n",
    "    tables=tables_to_copy,\n",
    "    overwrite=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
