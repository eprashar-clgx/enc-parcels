{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aec54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "from typing import Literal\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import shapely.geometry\n",
    "import duckdb\n",
    "from shapely.geometry import mapping\n",
    "from shapely.geometry import box\n",
    "import fiona\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# Import utility constants and functions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a70bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for the project\n",
    "#POC_DATASET = 'encumbered_parcels'\n",
    "#POC_TABLE = 'parcels'\n",
    "GEO_CRS = \"EPSG:4326\"\n",
    "PROJECTED_CRS = \"EPSG:3857\" \n",
    "ENCUMBRANCES = [\n",
    "    'roadways',\n",
    "    'railways',\n",
    "    'protected_lands',\n",
    "    'wetlands',\n",
    "    'transmission_lines',\n",
    "    ]\n",
    "EncumbranceType = Literal[\n",
    "    'roadways',\n",
    "    'railways',\n",
    "    'protected_lands',\n",
    "    'wetlands',\n",
    "    'transmission_lines',\n",
    "]\n",
    "\n",
    "STATE_ABBREV_TO_FIPS = {\n",
    "    'AK': '02',\n",
    "    'AL': '01',\n",
    "    'AZ': '04',\n",
    "    'AR': '05',\n",
    "    'CA': '06',\n",
    "    'CO': '08',\n",
    "    'CT': '09',\n",
    "    'DE': '10',\n",
    "    'FL': '12',\n",
    "    'GA': '13',\n",
    "    'HI': '15',\n",
    "    'ID': '16',\n",
    "    'IL': '17',\n",
    "    'IN': '18',\n",
    "    'IA': '19',\n",
    "    'KS': '20',\n",
    "    'KY': '21',\n",
    "    'LA': '22',\n",
    "    'ME': '23',\n",
    "    'MD': '24',\n",
    "    'MA': '25',\n",
    "    'MI': '26',\n",
    "    'MN': '27',\n",
    "    'MS': '28',\n",
    "    'MO': '29',\n",
    "    'MT': '30',\n",
    "    'NE': '31',\n",
    "    'NV': '32',\n",
    "    'NH': '33',\n",
    "    'NJ': '34',\n",
    "    'NM': '35',\n",
    "    'NY': '36',\n",
    "    'NC': '37',\n",
    "    'ND': '38',\n",
    "    'OH': '39',\n",
    "    'OK': '40',\n",
    "    'OR': '41',\n",
    "    'PA': '42',\n",
    "    'RI': '44',\n",
    "    'SC': '45',\n",
    "    'SD': '46',\n",
    "    'TN': '47',\n",
    "    'TX': '48',\n",
    "    'UT': '49',\n",
    "    'VT': '50',\n",
    "    'VA': '51',\n",
    "    'WA': '53',\n",
    "    'WV': '54',\n",
    "    'WI': '55',\n",
    "    'WY': '56',\n",
    "}\n",
    "# Define paths for local data\n",
    "LOCAL_DATA_FOLDER = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\" # This is the local path where data files are stored\n",
    "\n",
    "# Define paths for various data files\n",
    "COUNTY_DATA = r\"counties\\tl_2024_us_county\\tl_2024_us_county.shp\" # This is the path to the shapefile with county boundaries from CENSUS\n",
    "\n",
    "# Line datasets\n",
    "RAILWAYS_DATA = r\"NTAD_North_American_Rail_Network_Lines\\NARN.gdb\" # This is the path to the railways data from North American Rail Network (Dept. of Transportation)\n",
    "TRANSMISSION_LINES_DATA = r\"transmission_lines\\Transmission_Lines.shp\" # This is the path to the transmission lines data downloaded from Homeland Security's Data Downloads\n",
    "ROADWAYS_DATA = r\"NTAD_North_American_Roads\\North_American_Roads.shp\" # This is the path to the roadways data from North American Roads (Dept. of Transportation)\n",
    "\n",
    "# Polygon datasets\n",
    "PROTECTED_LANDS_NATIONAL = r\"protected_lands_national\\PADUS4_1VectorAnalysis_PADUS_Only\\PADUS4_1VectorAnalysis_PADUS_Only.gdb\" # This is the path to the protected lands data downloaded from PAD-US (Protected Areas Database of the US)\n",
    "WETLANDS = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\Wetlands\" # USGS Wetlands data is stored at a state-level granularity in this folder\n",
    "WETLAND_ATTRIBUTES = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\Wetlands\\NWI-Code-Definitions\\NWI-Code-Definitions\\NWI_Code_Definitions.gdb\"\n",
    "\n",
    "# Output paths\n",
    "PARQUET_INGESTION_PATH = r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\" # This is the path where the parquet files will be stored for ingestion into BigQuery\n",
    "WETLAND_COUNTY_FILES = os.path.join(PARQUET_INGESTION_PATH, \"wetland_county_level\") # This is the path where the county-level wetland files will be stored. These are then uploaded to BQ using code in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407205cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World.\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DuckDB doesn't have something along the lines of geopandas for CRS conversion\n",
    "# So check the CRS of each data source here and manually define it in config object below for later use\n",
    "# Sample code for roadways\n",
    "path = os.path.join(LOCAL_DATA_FOLDER, ROADWAYS_DATA)\n",
    "gdf = gpd.read_file(path)\n",
    "gdf.crs\n",
    "\n",
    "# Another way to do this is load the file in QGIS and copy paste the CRS from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dcf3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Config object to hold configuration settings\n",
    "# Projection conversions are super tricky to handle in DuckDb and need some manual QA \n",
    "# always_xy := TRUE is a duckdb-specific argument to ensure that the first coordinate is always treated as X (longitude) and the second as Y (latitude), regardless of the CRS definition.\n",
    "DATASET_CONFIG = {\n",
    "    'transmission_lines': {\n",
    "        'path': TRANSMISSION_LINES_DATA,\n",
    "        'source_crs': 'EPSG:3857', # Main pitfall of DudkDB is we have to manually set the source CRS. I did this using gdf.crs from the geopandas read_file method. X[east]. Y[north] \n",
    "        'force_xy': True,\n",
    "        'columns_to_drop': ['OBJECTID', 'SOURCE', 'SOURCEDATE', 'VAL_METHOD', 'VOLTAGE', 'INFERRED', 'SUB_1', 'SUB_2']\n",
    "    },\n",
    "    'railways': {\n",
    "        'path': RAILWAYS_DATA,\n",
    "        'read_kwargs': {'layer': 'North_American_Rail_Network_Lines'},\n",
    "        'source_crs': 'EPSG:3857', # Same note as in transmission lines \n",
    "        'force_xy': True,\n",
    "        'columns_to_drop': ['FRFRANODE', 'TOFRANODE', 'STFIPS', 'CNTYFIPS', 'STATEAB', 'COUNTRY',\n",
    "                            'FRADISTRCT', 'RROWNER1', 'RROWNER2', 'RROWNER3', 'TRKRGHTS1', 'TRKRGHTS2',\n",
    "                            'TRKRGHTS3', 'TRKRGHTS4', 'TRKRGHTS5', 'TRKRGHTS6', 'TRKRGHTS7', 'TRKRGHTS8',\n",
    "                            'TRKRGHTS9', 'DIVISION', 'SUBDIV', 'BRANCH', 'YARDNAME', 'PASSNGR', 'STRACNET',\n",
    "                            'TRACKS', 'NET', 'MILES', 'TIMEZONE', 'SHAPE_Length']\n",
    "    },\n",
    "    'roadways': {\n",
    "        'path': ROADWAYS_DATA,\n",
    "        'source_crs': 'EPSG:4326', # Lat North; Long East\n",
    "        'force_xy': True,\n",
    "        'filter_clause': \"COUNTRY = 2\", # Filter for USA roads\n",
    "        'columns_to_drop': ['DIR', 'LINKID', 'JURISCODE', 'ROADNUM', 'CLASS', 'NHS', 'COUNTRY']\n",
    "    },\n",
    "    # Projection information: https://www.fws.gov/node/264848\n",
    "   'wetlands': {\n",
    "        'gdb_config': lambda state: {'folder': WETLANDS, 'subfolder': f\"{state}_geodatabase_wetlands\", 'gdb_name': f\"{state}_geodatabase_wetlands.gdb\"},\n",
    "        'source_crs_lookup': lambda state: {\n",
    "            'AK': ('EPSG:3338', True),\n",
    "            'HI': ('PROJCS[\"NAD_1983_Albers\",BASEGEOGCRS[\"NAD83\",DATUM[\"North American Datum 1983\",ELLIPSOID[\"GRS 1980\",6378137,298.257222101,LENGTHUNIT[\"metre\",1]],ID[\"EPSG\",6269]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"Degree\",0.0174532925199433]]],CONVERSION[\"unnamed\",METHOD[\"Albers Equal Area\",ID[\"EPSG\",9822]],PARAMETER[\"Latitude of false origin\",3,ANGLEUNIT[\"Degree\",0.0174532925199433],ID[\"EPSG\",8821]],PARAMETER[\"Longitude of false origin\",-157,ANGLEUNIT[\"Degree\",0.0174532925199433],ID[\"EPSG\",8822]],PARAMETER[\"Latitude of 1st standard parallel\",8,ANGLEUNIT[\"Degree\",0.0174532925199433],ID[\"EPSG\",8823]],PARAMETER[\"Latitude of 2nd standard parallel\",18,ANGLEUNIT[\"Degree\",0.0174532925199433],ID[\"EPSG\",8824]],PARAMETER[\"Easting at false origin\",0,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8826]],PARAMETER[\"Northing at false origin\",0,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8827]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]]]', True)\n",
    "        }.get(state, ('EPSG:5070', True))\n",
    "    },\n",
    "    'protected_lands_national': {\n",
    "        'path': PROTECTED_LANDS_NATIONAL,\n",
    "        'read_kwargs': {'layer':'PADUS4_1VectorAnalysis_PADUS_Only_Simp_SingP'},\n",
    "        'source_crs': 'EPSG:5070', # NAD83 Albers\n",
    "        'force_xy': True,\n",
    "        'columns_to_drop': ['FID_GAP_Sts14_13_12_12_11', 'Agg_Src', 'ShL_ShA', 'DupShL_ShA', \n",
    "                            'RevOID', 'Shp_AreaDup', 'GIS_Acres', 'BndryName', 'BndryExten', \n",
    "                            'BndryID', 'GIS_AcrsDb', 'InPoly_FID', 'SimPgnFlag', \n",
    "                            'MaxSimpTol', 'MinSimpTol', 'Shape_Length'\n",
    "                            ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4085c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processor class to load a dataset, make valid geometries, transform CRS, drop columns and save to Parquet\n",
    "# This class handles both national datasets and state-specific datasets based on the provided configuration\n",
    "# Data can be saved for state even if the dataset is national (e.g. roads)\n",
    "# To save datasets at a state level, provide the state abbreviation (e.g. 'CA') when initializing the class\n",
    "# This class leverages county polygons from CENSUS to then assign fips codes to the layer for the assigned state\n",
    "class GeospatialProcessor:\n",
    "    \"\"\"\n",
    "    A class to process large geospatial datasets using a memory-efficient,\n",
    "    config-driven DuckDB pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, state=None):\n",
    "        self.dataset = dataset\n",
    "        self.state = state\n",
    "        self.config = DATASET_CONFIG.get(dataset)\n",
    "        if not self.config:\n",
    "            raise ValueError(f\"Unsupported dataset: {self.dataset}\")\n",
    "\n",
    "        filename_prefix = f\"{self.state}_{self.dataset}\" if self.state else self.dataset\n",
    "        self.db_file = os.path.join(PARQUET_INGESTION_PATH, f\"{filename_prefix}.duckdb\")\n",
    "        self.output_parquet_path = os.path.join(PARQUET_INGESTION_PATH, f\"{filename_prefix}.parquet\")\n",
    "        self.target_crs = GEO_CRS\n",
    "        \n",
    "        os.makedirs(PARQUET_INGESTION_PATH, exist_ok=True)\n",
    "        self.con = self._connect_db()\n",
    "\n",
    "    def _connect_db(self):\n",
    "        print(f\"Connecting to DuckDB database: {self.db_file}\")\n",
    "        con = duckdb.connect(database=self.db_file, read_only=False)\n",
    "        con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "        print(\"DuckDB connection established and spatial extension loaded.\")\n",
    "        return con\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Dispatcher method to run the correct pipeline based on config.\"\"\"\n",
    "        try:\n",
    "            if self.state:\n",
    "                self._run_state_pipeline()\n",
    "            else:\n",
    "                self._run_national_pipeline()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR: An error occurred during processing for {self.dataset}: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    # This function deals with DuckDB specific quirks around reading different file formats\n",
    "    # gdb files typically seem to have geometry stored in a column named 'shape' while shapefiles have it in geometry or geom\n",
    "    # The quirk is using \"keep_wkb=TRUE\" in the ST_Read function changes geometry column name to wkb_geometry for shape formats but not so for gdb files\n",
    "    def _create_standardized_source_table(self, full_path, output_table_name, layer_name=None):\n",
    "        \"\"\"\n",
    "        Reads a source file and creates a table with a standardized 'geom_wkb' column.\n",
    "        \"\"\"\n",
    "        print(f\"Reading source file '{os.path.basename(full_path)}' and standardizing schema...\")\n",
    "        layer_sql = f\", layer='{layer_name}'\" if layer_name else \"\"\n",
    "        \n",
    "        self.con.execute(f\"CREATE OR REPLACE TEMP VIEW temp_schema_view AS SELECT * FROM ST_Read('{full_path}'{layer_sql}, keep_wkb=TRUE);\")\n",
    "        df_schema = self.con.execute(\"DESCRIBE temp_schema_view;\").fetchdf()\n",
    "        \n",
    "        # For new datasets, we may need to expand this list of possible geometry column names\n",
    "        geom_col_name = None\n",
    "        for col_name in df_schema['column_name']:\n",
    "            if col_name.lower() in ['shape', 'wkb_geometry', 'geometry', 'geom']:\n",
    "                geom_col_name = col_name\n",
    "                break\n",
    "        \n",
    "        if not geom_col_name:\n",
    "            raise ValueError(f\"Could not find a recognized geometry column in {full_path}\")\n",
    "            \n",
    "        print(f\"-> Detected raw geometry column: '{geom_col_name}'\")\n",
    "\n",
    "        self.con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {output_table_name} AS\n",
    "            SELECT \n",
    "                * EXCLUDE (\"{geom_col_name}\"), \n",
    "                \"{geom_col_name}\" AS geom_wkb\n",
    "            FROM ST_Read('{full_path}'{layer_sql}, keep_wkb=TRUE);\n",
    "        \"\"\")\n",
    "        print(f\"-> Standardized raw data loaded into '{output_table_name}' table.\")\n",
    "\n",
    "    # Make valid geometries, filter out null values, transform to target CRS (EPSG 4326), \n",
    "    # drop unwanted columns and save to Parquet\n",
    "    def _run_national_pipeline(self):\n",
    "        \"\"\"Executes the processing pipeline for a single national dataset.\"\"\"\n",
    "        print(f\"\\n--- Running National Pipeline for '{self.dataset}' ---\")\n",
    "        \n",
    "        full_path = os.path.join(LOCAL_DATA_FOLDER, self.config['path'])\n",
    "        layer_name = self.config.get('read_kwargs', {}).get('layer')\n",
    "        source_crs = self.config['source_crs']\n",
    "        force_xy_flag = self.config.get('force_xy', False)\n",
    "        transform_params = \", always_xy := TRUE\" if force_xy_flag else \"\"\n",
    "\n",
    "        self._create_standardized_source_table(full_path, 'raw_source_data', layer_name)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            CREATE OR REPLACE TABLE processed_data AS\n",
    "            WITH casted_data AS (\n",
    "                SELECT *, TRY_CAST(geom_wkb AS GEOMETRY) as geom_obj FROM raw_source_data\n",
    "            ),\n",
    "            validated_data AS (\n",
    "                SELECT *, ST_MakeValid(geom_obj) as validated_geom FROM casted_data \n",
    "                WHERE validated_geom IS NOT NULL AND NOT ST_IsEmpty(validated_geom)\n",
    "            ),\n",
    "            transformed_data AS (\n",
    "                SELECT *, ST_Transform(validated_geom, '{source_crs}', '{self.target_crs}'{transform_params}) as transformed_geom\n",
    "                FROM validated_data\n",
    "            )\n",
    "            SELECT * EXCLUDE (geom_wkb, geom_obj, validated_geom), transformed_geom as geometry \n",
    "            FROM transformed_data;\n",
    "        \"\"\"\n",
    "        self.con.execute(query)\n",
    "\n",
    "        current_table = 'processed_data'\n",
    "        if self.config.get('filter_clause'):\n",
    "            self.con.execute(f\"CREATE OR REPLACE TABLE filtered_data AS SELECT * FROM {current_table} WHERE {self.config['filter_clause']}\")\n",
    "            current_table = 'filtered_data'\n",
    "\n",
    "        columns_to_drop = self.config.get('columns_to_drop', [])\n",
    "        all_columns = self.con.execute(f\"DESCRIBE {current_table}\").fetchdf()['column_name'].tolist()\n",
    "        final_columns = [f'\"{col}\"' for col in all_columns if col not in columns_to_drop and col != 'geometry']\n",
    "        final_select_sql = \", \".join(final_columns)\n",
    "\n",
    "        save_query = f\"\"\"\n",
    "        COPY (\n",
    "            SELECT {final_select_sql}, ST_AsWKB(geometry) AS geometry\n",
    "            FROM {current_table}\n",
    "        ) TO '{self.output_parquet_path}' (FORMAT PARQUET);\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Executing final query and saving to Parquet...\")\n",
    "        self.con.execute(save_query)\n",
    "        print(f\"-> SUCCESS: Saved processed data to '{self.output_parquet_path}'\")\n",
    "    \n",
    "    # Everything in national-level pipeline plus county fips assignment using spatial join on CENSUS boundaries \n",
    "    def _run_state_pipeline(self):\n",
    "        \"\"\"Executes the multi-step pipeline for state-based data.\"\"\"\n",
    "        print(f\"\\n--- Running State-Level Pipeline for '{self.dataset}' in State: '{self.state}' ---\")\n",
    "        self._load_source_data()\n",
    "        self._load_boundaries()\n",
    "        self._perform_spatial_join()\n",
    "        self._save_state_level_result()\n",
    "        self._cleanup_intermediate_tables()\n",
    "        print(f\"-> SUCCESS: End-to-end processing for '{self.dataset}' in {self.state} complete.\")\n",
    "\n",
    "    # Only for state-level processing\n",
    "    def _load_source_data(self):\n",
    "        \"\"\"Loads and validates the source data for the state-level pipeline.\"\"\"\n",
    "        layer_name = self.config.get('read_kwargs', {}).get('layer')\n",
    "        \n",
    "        # This logic is now cleaner and more robust\n",
    "        if 'gdb_config' in self.config:\n",
    "            gdb_config = self.config['gdb_config'](self.state)\n",
    "            full_path = os.path.join(LOCAL_DATA_FOLDER, gdb_config['folder'], gdb_config['subfolder'], gdb_config['gdb_name'])\n",
    "            if not layer_name:\n",
    "                layer_name = self._find_largest_layer(full_path)\n",
    "            # Unpack the CRS code and the flip flag from the config\n",
    "            source_crs, force_xy_flag = self.config['source_crs_lookup'](self.state)\n",
    "        else:\n",
    "            full_path = os.path.join(LOCAL_DATA_FOLDER, self.config['path'])\n",
    "            source_crs = self.config['source_crs']\n",
    "            force_xy_flag = self.config.get('force_xy', False)\n",
    "\n",
    "        # Dynamically build the transformation parameter string\n",
    "        transform_params = \", always_xy := TRUE\" if force_xy_flag else \"\"\n",
    "\n",
    "        self._create_standardized_source_table(full_path, 'raw_source_data', layer_name)\n",
    "        \n",
    "        print(f\"Step 1: Transforming and validating source data '{self.dataset}'\")\n",
    "        query = f\"\"\"\n",
    "            CREATE OR REPLACE TABLE source_data AS\n",
    "            WITH casted_data AS (\n",
    "                SELECT *, TRY_CAST(geom_wkb AS GEOMETRY) as geom_obj FROM raw_source_data\n",
    "            ),\n",
    "            validated_data AS (\n",
    "                SELECT *, ST_MakeValid(geom_obj) as validated_geom FROM casted_data \n",
    "                WHERE validated_geom IS NOT NULL\n",
    "            )\n",
    "            SELECT \n",
    "                * EXCLUDE (geom_wkb, geom_obj, validated_geom),\n",
    "                ST_Transform(validated_geom, '{source_crs}', '{self.target_crs}'{transform_params}) as geometry\n",
    "            FROM validated_data;\n",
    "        \"\"\"\n",
    "        self.con.execute(query)\n",
    "        count = self.con.execute(\"SELECT COUNT(*) FROM source_data\").fetchone()[0]\n",
    "        print(f\"-> Loaded and validated {count:,} features into 'source_data' table.\")\n",
    "\n",
    "    # Loads census county boundaries, filters for the state, transforms to target CRS\n",
    "    # CENSUS CRS IS MANUALLY DEFINED HERE\n",
    "    def _load_boundaries(self):\n",
    "        print(\"Step 2: Loading county boundaries...\")\n",
    "        boundaries_path = os.path.join(LOCAL_DATA_FOLDER, COUNTY_DATA)\n",
    "        self._create_standardized_source_table(boundaries_path, 'raw_boundaries_data')\n",
    "        \n",
    "        boundary_crs = 'EPSG:4269' \n",
    "        state_fips = STATE_ABBREV_TO_FIPS.get(self.state)\n",
    "        if not state_fips: raise ValueError(f\"State FIPS code not found for {self.state}\")\n",
    "\n",
    "        # The county data uses a standard CRS that does not require axis flipping.\n",
    "        query = f\"\"\"\n",
    "            CREATE OR REPLACE TABLE county_boundaries AS\n",
    "            WITH casted_data AS (\n",
    "                SELECT *, TRY_CAST(geom_wkb AS GEOMETRY) as geom_obj FROM raw_boundaries_data\n",
    "            )\n",
    "            SELECT \n",
    "                GEOID as fips, \n",
    "                NAME, \n",
    "                NAMELSAD, \n",
    "                ST_Transform(geom_obj, '{boundary_crs}', '{self.target_crs}') AS geometry\n",
    "            FROM casted_data \n",
    "            WHERE STATEFP = '{state_fips}';\n",
    "        \"\"\"\n",
    "        self.con.execute(query)\n",
    "        count = self.con.execute(\"SELECT COUNT(*) FROM county_boundaries\").fetchone()[0]\n",
    "        print(f\"-> Loaded {count} county boundaries for state {self.state}.\")\n",
    "\n",
    "    # Only for state-level processing\n",
    "    def _perform_spatial_join(self):\n",
    "        print(\"Step 3: Performing spatial join...\")\n",
    "        self.con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE data_by_county AS\n",
    "            SELECT s.*, b.fips, b.NAME, b.NAMELSAD\n",
    "            FROM source_data s\n",
    "            JOIN county_boundaries b ON ST_Intersects(s.geometry, b.geometry);\n",
    "        \"\"\")\n",
    "        count = self.con.execute(\"SELECT COUNT(*) FROM data_by_county\").fetchone()[0]\n",
    "        print(f\"-> Spatial join complete. {count:,} features assigned to counties.\")\n",
    "\n",
    "    # Only for state-level processing\n",
    "    def _save_state_level_result(self):\n",
    "        \"\"\"Saves the final joined data, handling potential memory errors.\"\"\"\n",
    "        print(\"Step 4: Saving final state-level data...\")\n",
    "        save_query = f\"\"\"\n",
    "        COPY (\n",
    "            SELECT * REPLACE (ST_AsWKB(geometry) AS geometry)\n",
    "            FROM data_by_county\n",
    "        ) TO '{self.output_parquet_path}' (FORMAT PARQUET);\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.con.execute(save_query)\n",
    "            print(f\"-> Final state-level data saved to {self.output_parquet_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"-> WARNING: Failed to save final data directly due to error: {e}\")\n",
    "            debug_db_path = self.db_file.replace(\".duckdb\", \"_error_debug.duckdb\")\n",
    "            print(f\"-> Preserving database state for debugging at: {debug_db_path}\")\n",
    "            self.con.close()\n",
    "            self.con = None \n",
    "            shutil.copy2(self.db_file, debug_db_path)\n",
    "            raise\n",
    "    \n",
    "    # Only for state-level processing\n",
    "    def _cleanup_intermediate_tables(self):\n",
    "        print(\"Step 5: Cleaning up intermediate tables...\")\n",
    "        tables_to_drop = ['source_data', 'county_boundaries', 'raw_source_data', 'raw_boundaries_data', 'processed_data', 'filtered_data']\n",
    "        for table in tables_to_drop:\n",
    "            self.con.execute(f\"DROP TABLE IF EXISTS {table};\")\n",
    "        print(\"-> Intermediate tables dropped.\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the DuckDB connection and cleans up the database file.\"\"\"\n",
    "        if self.con:\n",
    "            self.con.close()\n",
    "        # Only remove the file if the process was successful and it's a national file\n",
    "        if not self.state and os.path.exists(self.db_file):\n",
    "             os.remove(self.db_file)\n",
    "        print(\"DuckDB connection closed.\")\n",
    "    \n",
    "    # Identifies the largest layer in a gdb file      \n",
    "    # Relevant for wetlands data where layer name might not be known upfront  \n",
    "    def _find_largest_layer(self, gdb_path):\n",
    "        with fiona.open(gdb_path) as collection:\n",
    "            return max(fiona.listlayers(gdb_path), key=lambda layer: len(fiona.open(gdb_path, layer=layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to DuckDB database: C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\\HI_wetlands.duckdb\n",
      "DuckDB connection established and spatial extension loaded.\n",
      "\n",
      "--- Running State-Level Pipeline for 'wetlands' in State: 'HI' ---\n",
      "Reading source file 'HI_geodatabase_wetlands.gdb' and standardizing schema...\n",
      "-> Detected raw geometry column: 'Shape'\n",
      "-> Standardized raw data loaded into 'raw_source_data' table.\n",
      "Step 1: Transforming and validating source data 'wetlands'\n",
      "-> Loaded and validated 9,191 features into 'source_data' table.\n",
      "Step 2: Loading county boundaries...\n",
      "Reading source file 'tl_2024_us_county.shp' and standardizing schema...\n",
      "-> Detected raw geometry column: 'wkb_geometry'\n",
      "-> Standardized raw data loaded into 'raw_boundaries_data' table.\n",
      "-> Loaded 5 county boundaries for state HI.\n",
      "Step 3: Performing spatial join...\n",
      "-> Spatial join complete. 9,197 features assigned to counties.\n",
      "Step 4: Saving final state-level data...\n",
      "-> Final state-level data saved to C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\\HI_wetlands.parquet\n",
      "Step 5: Cleaning up intermediate tables...\n",
      "-> Intermediate tables dropped.\n",
      "-> SUCCESS: End-to-end processing for 'wetlands' in HI complete.\n",
      "DuckDB connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Execute the pipeline for a national dataset and state\n",
    "processor = GeospatialProcessor(\n",
    "    dataset='wetlands',\n",
    "    state='HI' \n",
    "    )\n",
    "processor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7267e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main processing pipeline for wetlands data\n",
    "# This script processes wetlands data for specified states, handling retries for specific states if needed.\n",
    "state_list = list(STATE_ABBREV_TO_FIPS.keys())\n",
    "\n",
    "# OR use specific states for retry\n",
    "state_retry = ['AK']\n",
    "\n",
    "# Loop through states as needed\n",
    "for state in state_retry:\n",
    "    # Skip if parquet file already exists for state\n",
    "    if os.path.exists(os.path.join(PARQUET_INGESTION_PATH, f\"{state}_wetlands.parquet\")):\n",
    "        print(f\"Skipping state {state} as it has already been processed.\")\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            # 1. Instantiate the processor for the desired dataset and state\n",
    "            processor = GeospatialProcessor(\n",
    "                dataset='wetlands', \n",
    "                state=state\n",
    "                )  \n",
    "            # 2. Run the entire pipeline\n",
    "            processor.run()\n",
    "        except Exception as e:\n",
    "            print(f\"The main script caught an error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Preserving database state for debugging at: C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\\ak_wetlands_error_debug.duckdb\n",
      "-> Final state-level data saved as C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\\ak_wetlands_error_debug.duckdb\n"
     ]
    }
   ],
   "source": [
    "# In case console crashes and we want to retrieve the last state of the database for debugging\n",
    "state = 'ak'\n",
    "db_file = os.path.join(PARQUET_INGESTION_PATH, f\"{state}_wetlands.duckdb\")\n",
    "debug_db_path = db_file.replace(\".duckdb\", \"_error_debug.duckdb\")\n",
    "print(f\"-> Preserving database state for debugging at: {debug_db_path}\")\n",
    "try:\n",
    "    # Now copy the file\n",
    "    shutil.copy2(db_file, debug_db_path)\n",
    "except Exception as copy_e:\n",
    "    print(f\"-> ERROR: Could not copy debug database file: {copy_e}\")\n",
    "    raise\n",
    "print(f\"-> Final state-level data saved as {debug_db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9dce57",
   "metadata": {},
   "source": [
    "<u>NOTES</u>:\n",
    "\n",
    "1. Wetlands has a more extensive workflow as a part of which, we need to merge certain attributes to data and save parquet files at a county-level for ingestion in BQ from GCS\n",
    "2. All other data sources can straight-away be uploaded to GCS (either national file or state level file). Note if national-file is uploaded, fips assignment will need to be done in BQ using intersection with county boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25163f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once a state level wetlands file is created, merge the attributes\n",
    "# This function merges wetlands attributes into the state-level wetlands data files.\n",
    "def merge_wetland_attributes(state_list):\n",
    "    \"\"\"\n",
    "    Merges wetlands attributes into the state-level wetlands data files.\n",
    "    This function creates a persistent DuckDB file for the attributes once,\n",
    "    and then attaches that file to process each state's data.\n",
    "\n",
    "    Args:\n",
    "        state_list (list): A list of state abbreviations to process (e.g., ['AK', 'MT']).\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Wetland Attribute Merge Process ---\")\n",
    "\n",
    "    # --- 1. Create or Verify the Persistent Attributes Database ---\n",
    "    attributes_db_file = os.path.join(PARQUET_INGESTION_PATH, \"wetlands_attributes.duckdb\")\n",
    "    \n",
    "    if not os.path.exists(attributes_db_file):\n",
    "        print(f\"Attributes database not found. Creating at: {attributes_db_file}\")\n",
    "        try:\n",
    "            con = duckdb.connect(database=attributes_db_file, read_only=False)\n",
    "            con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "            load_attributes_query = f\"\"\"\n",
    "            CREATE OR REPLACE TABLE wetland_attributes AS\n",
    "            SELECT\n",
    "                ATTRIBUTE, SUBSYSTEM_NAME, CLASS_NAME, SUBCLASS_NAME,\n",
    "                SPLIT_CLASS_NAME, WATER_REGIME_NAME, WATER_REGIME_SUBGROUP\n",
    "            FROM ST_Read('{WETLAND_ATTRIBUTES}');\n",
    "            \"\"\"\n",
    "            con.execute(load_attributes_query)\n",
    "            print(\"-> Successfully created and loaded wetlands attributes database.\")\n",
    "        except Exception as e:\n",
    "            print(f\"-> FATAL ERROR: Could not create attributes database: {e}\")\n",
    "            return # Stop the process if attributes can't be loaded\n",
    "        finally:\n",
    "            if 'con' in locals() and con:\n",
    "                con.close()\n",
    "    else:\n",
    "        print(f\"Using existing attributes database: {attributes_db_file}\")\n",
    "\n",
    "    # --- 2. Loop through each state and process its file ---\n",
    "    for state in state_list:\n",
    "        print(f\"\\n--- Processing State: {state} ---\")\n",
    "        \n",
    "        source_parquet = os.path.join(PARQUET_INGESTION_PATH, f\"{state}_wetlands.parquet\")\n",
    "        source_duckdb = os.path.join(PARQUET_INGESTION_PATH, f\"{state}_wetlands_error_debug.duckdb\")\n",
    "        \n",
    "        if os.path.exists(source_parquet):\n",
    "            print(f\"Found source Parquet file: {source_parquet}\")\n",
    "            # For Parquet, we can use an in-memory DB and attach the attributes file\n",
    "            con = duckdb.connect(database=':memory:', read_only=False)\n",
    "            try:\n",
    "                con.execute(f\"ATTACH '{attributes_db_file}' AS attributes_db (READ_ONLY);\")\n",
    "                temp_output_file = source_parquet.replace(\".parquet\", \"_merged_temp.parquet\")\n",
    "                \n",
    "                join_query = f\"\"\"\n",
    "                COPY (\n",
    "                    SELECT s.*, a.* EXCLUDE (ATTRIBUTE)\n",
    "                    FROM read_parquet('{source_parquet}') s\n",
    "                    LEFT JOIN attributes_db.wetlands_attributes a ON s.ATTRIBUTE = a.ATTRIBUTE\n",
    "                ) TO '{temp_output_file}' (FORMAT PARQUET);\n",
    "                \"\"\"\n",
    "                \n",
    "                print(f\"-> Performing join and writing to temporary file...\")\n",
    "                con.execute(join_query)\n",
    "                \n",
    "                os.remove(source_parquet)\n",
    "                os.rename(temp_output_file, source_parquet)\n",
    "                print(f\"-> SUCCESS: Replaced original file with merged data at: {source_parquet}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"-> ERROR processing Parquet for state {state}: {e}\")\n",
    "                if os.path.exists(temp_output_file):\n",
    "                    os.remove(temp_output_file)\n",
    "            finally:\n",
    "                if con: con.close()\n",
    "\n",
    "        elif os.path.exists(source_duckdb):\n",
    "            print(f\"Found source DuckDB debug file: {source_duckdb}\")\n",
    "            # For DuckDB files, connect directly and attach the attributes file\n",
    "            con = duckdb.connect(database=source_duckdb, read_only=False)\n",
    "            try:\n",
    "                con.execute(f\"ATTACH '{attributes_db_file}' AS attributes_db (READ_ONLY);\")\n",
    "                \n",
    "                print(\"-> Performing join and updating 'data_by_county' table in-place...\")\n",
    "                con.execute(\"\"\"\n",
    "                    CREATE OR REPLACE TABLE merged_data AS\n",
    "                    SELECT s.*, a.* EXCLUDE (ATTRIBUTE)\n",
    "                    FROM data_by_county s\n",
    "                    LEFT JOIN attributes_db.wetlands_attributes a ON s.ATTRIBUTE = a.ATTRIBUTE;\n",
    "                \"\"\")\n",
    "                \n",
    "                con.execute(\"DROP TABLE data_by_county;\")\n",
    "                con.execute(\"ALTER TABLE merged_data RENAME TO data_by_county;\")\n",
    "                \n",
    "                print(f\"-> SUCCESS: Updated 'data_by_county' table in {source_duckdb}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"-> ERROR processing DuckDB file for state {state}: {e}\")\n",
    "            finally:\n",
    "                if con:\n",
    "                    con.execute(\"DETACH attributes_db;\")\n",
    "                    con.close()\n",
    "\n",
    "        else:\n",
    "            print(f\"-> No source file found for state {state}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "    print(\"\\n--- Attribute Merge Process Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the merge_wetland_attributes function for all states\n",
    "# This will merge attributes into the state-level wetlands data files.\n",
    "state_list = list(STATE_ABBREV_TO_FIPS.keys())\n",
    "state_retry = ['AK']\n",
    "for state in state_retry: # Check which states need to be reprocessed\n",
    "    try:# Process all counties for the given state\n",
    "        merge_wetland_attributes(state_list=[state])\n",
    "    except Exception as e:\n",
    "        print(f\"The main script caught an error while processing state {state}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "367eadd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a single county's data in chunks\n",
    "# This function is designed to handle large datasets efficiently by processing them in smaller chunks.\n",
    "def extract_single_county_chunked(\n",
    "        con,\n",
    "        from_clause,\n",
    "        state_abbrev, \n",
    "        fips_code,\n",
    "        chunk_size=40000):\n",
    "    \"\"\"\n",
    "     Args:\n",
    "        con: An active DuckDB connection object.\n",
    "        state_abbrev (str): The two-letter state abbreviation.\n",
    "        fips_code (str): The specific county FIPS code to extract.\n",
    "        chunk_size (int): The number of rows to process in each chunk.\n",
    "    \"\"\"\n",
    "    print(f\"  -> Starting extraction for FIPS: {fips_code}\")\n",
    "    output_file = os.path.join(WETLAND_COUNTY_FILES, f\"{state_abbrev}_{fips_code}_wetlands.parquet\")\n",
    "    \n",
    "    # This query selects all data for the county. \n",
    "    query = f\"SELECT \\\n",
    "        NWI_ID, \\\n",
    "        '{state_abbrev}' AS state,\\\n",
    "        fips,\\\n",
    "        NAME AS county,\\\n",
    "        NAMELSAD AS county_full_name,\\\n",
    "        ATTRIBUTE, \\\n",
    "        WETLAND_TYPE,\\\n",
    "        ACRES,\\\n",
    "        SUBSYSTEM_NAME, \\\n",
    "        CLASS_NAME,\\\n",
    "        SUBCLASS_NAME,\\\n",
    "        SPLIT_CLASS_NAME,\\\n",
    "        WATER_REGIME_NAME,\\\n",
    "        WATER_REGIME_SUBGROUP,\\\n",
    "        ST_AsWKB(geometry) AS geometry\\\n",
    "        FROM {from_clause} WHERE fips = '{fips_code}'\"\n",
    "    \n",
    "    # Use fetch_record_batch for maximum efficiency with pyarrow\n",
    "    reader = con.execute(query).fetch_record_batch(chunk_size)\n",
    "    print(f\"  -> Query executed. Processing data for FIPS: {fips_code} in chunks of {chunk_size} rows.\")\n",
    "\n",
    "    # Initialize a counter for total rows processed\n",
    "    writer = 0\n",
    "    total_rows = 0\n",
    "    \n",
    "    try:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i == 0:\n",
    "                print(f\"Processing chunk {i+1}...\")\n",
    "                # For the first chunk, create the Parquet file and writer\n",
    "                writer = pq.ParquetWriter(output_file, chunk.schema)\n",
    "            else:\n",
    "                print(f\"Processing chunk {i+1}...\")\n",
    "\n",
    "            writer.write_batch(chunk)\n",
    "            total_rows += len(chunk)\n",
    "\n",
    "        if total_rows > 0:\n",
    "            print(f\"  -> SUCCESS: Wrote {total_rows:,} rows to: {output_file}\")\n",
    "        else:\n",
    "            print(f\"  -> No data found for FIPS {fips_code}. No file was created.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> ERROR for FIPS {fips_code}: {e}\")\n",
    "        # If an error occurs, clean up the partially written file\n",
    "        if writer:\n",
    "            writer.close()\n",
    "            os.remove(output_file)\n",
    "        raise\n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "\n",
    "########################################################\n",
    "def process_all_counties(state_abbrev):\n",
    "    \"\"\"\n",
    "    Orchestrates the extraction process for all counties in a state.\n",
    "\n",
    "    Args:\n",
    "        state_abbrev (str): Two-letter state abbreviation (e.g., 'AK').\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Final Extraction Process for State: {state_abbrev} ---\")\n",
    "    \n",
    "    # --- 1. Determine which source file to use ---\n",
    "    source_parquet = os.path.join(PARQUET_INGESTION_PATH, f\"{state_abbrev}_wetlands.parquet\")\n",
    "    source_duckdb = os.path.join(PARQUET_INGESTION_PATH, f\"{state_abbrev}_wetlands_error_debug.duckdb\")\n",
    "    \n",
    "    source_path = None\n",
    "    #source_is_parquet = False\n",
    "    con = None\n",
    "    from_clause = None\n",
    "\n",
    "    if os.path.exists(source_parquet):\n",
    "        print(f\"Found Parquet source file: {source_parquet}\")\n",
    "        source_path = source_parquet\n",
    "        #source_is_parquet = True\n",
    "        con = duckdb.connect(read_only=False) # Use in-memory DB for reading Parquet\n",
    "        from_clause = f\"read_parquet('{source_path}')\"\n",
    "    elif os.path.exists(source_duckdb):\n",
    "        print(f\"Parquet file not found. Found DuckDB debug file: {source_duckdb}\")\n",
    "        source_path = source_duckdb\n",
    "        con = duckdb.connect(database=source_path, read_only=False)\n",
    "        from_clause = \"data_by_county\"\n",
    "    else:\n",
    "        print(f\"ERROR: No source file (.parquet or .duckdb) found for state {state_abbrev}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Connect and get list of counties ---\n",
    "    try:\n",
    "        con.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "        print(\"Fetching list of all counties from source...\")\n",
    "        fips_codes_result = con.execute(f\"SELECT DISTINCT fips FROM {from_clause} ORDER BY fips;\").fetchall()\n",
    "        fips_codes = [fips[0] for fips in fips_codes_result]\n",
    "        print(f\"Found {len(fips_codes)} counties to process.\")\n",
    "\n",
    "        # --- 3. Loop through each county and process it ---\n",
    "        for fips_code in fips_codes:\n",
    "            extract_single_county_chunked(\n",
    "                con, \n",
    "                from_clause, \n",
    "                state_abbrev, \n",
    "                fips_code\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"A critical error occurred during the extraction process: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "        print(\"\\n--- Extraction Process Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute process by county \n",
    "state_list = list(STATE_ABBREV_TO_FIPS.keys())\n",
    "state_retry = ['AK']  # List of states to retry processing\n",
    "for state in state_retry:\n",
    "    try:\n",
    "        process_all_counties(state_abbrev=state)\n",
    "    except Exception as e:\n",
    "        print(f\"The main script caught an error while processing state {state}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d3b5b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials file is older than 24 hours. Re-authenticating...\n",
      "Trying reauthentication on gcloud server using shell command...\n",
      "Login window opened...please complete authentication\n",
      "Waiting for credentials file to update...\n",
      "Authentication confirmed! Credentials file updated.\n"
     ]
    }
   ],
   "source": [
    "# Uploading to GCS\n",
    "# First, validate the authentication token\n",
    "CREDENTIALS_PATH =  r\"C:\\Users\\eprashar\\AppData\\Roaming\\gcloud\\application_default_credentials.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(CREDENTIALS_PATH)\n",
    "\n",
    "# Verify credentials\n",
    "utils.check_and_authenticate(CREDENTIALS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4f02b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for GCS upload\n",
    "GIS_PROJECT = 'clgx-gis-app-dev-06e3'\n",
    "BUCKET = 'geospatial-projects'\n",
    "DATASET = 'infra_parcels'\n",
    "\n",
    "# Function to upload county-level Parquet files to Google Cloud Storage\n",
    "def upload_to_gcs(\n",
    "        bucket_name,\n",
    "        source_folder, \n",
    "        destination_folder,\n",
    "        remove_local_files=False):\n",
    "    \"\"\"\n",
    "    Uploads all Parquet files from a local folder to a Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        source_folder (str): Local folder containing Parquet files.\n",
    "        destination_folder (str): Destination folder in the GCS bucket.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith('.parquet') or filename.__contains__('_error_debug.duckdb'):\n",
    "            print(f\"Preparing to upload {filename}...\")\n",
    "            try:\n",
    "                local_file_path = os.path.join(source_folder, filename)\n",
    "                blob = bucket.blob(os.path.join(destination_folder, filename))\n",
    "                blob.upload_from_filename(local_file_path)\n",
    "                print(f\"Uploaded {filename} to gs://{bucket_name}/{destination_folder}/\")\n",
    "                # Optionally, remove the local file after upload\n",
    "                if remove_local_files:\n",
    "                    os.remove(local_file_path)\n",
    "                    print(f\"Removed local file: {local_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {filename}: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "14909ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to upload MN_wetlands_error_debug.duckdb...\n",
      "Uploaded MN_wetlands_error_debug.duckdb to gs://geospatial-projects/infra_parcels/wetlands_v2/state//\n",
      "Preparing to upload TX_wetlands_error_debug.duckdb...\n",
      "Uploaded TX_wetlands_error_debug.duckdb to gs://geospatial-projects/infra_parcels/wetlands_v2/state//\n"
     ]
    }
   ],
   "source": [
    "# Upload all county-level Parquet files to GCS\n",
    "upload_to_gcs(\n",
    "    bucket_name=BUCKET,\n",
    "    source_folder=r\"C:\\Users\\eprashar\\OneDrive - CoreLogic Solutions, LLC\\github\\jan_25_proj_infra_parcels\\data\\ingestion_parquets\",\n",
    "    destination_folder=f\"{DATASET}/wetlands_v2/state/\",\n",
    "    remove_local_files=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
