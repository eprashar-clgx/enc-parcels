{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80e36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery, storage \n",
    "from google.cloud.exceptions import GoogleCloudError\n",
    "from utils import check_and_authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970d53f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials file is older than 24 hours. Re-authenticating...\n",
      "Trying reauthentication on gcloud server using shell command...\n",
      "Login window opened...please complete authentication\n",
      "Waiting for credentials file to update...\n",
      "Authentication confirmed! Credentials file updated.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# First, validate the authentication token\n",
    "CREDENTIALS_PATH =  r\"C:\\Users\\eprashar\\AppData\\Roaming\\gcloud\\application_default_credentials.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(CREDENTIALS_PATH)\n",
    "check_and_authenticate(CREDENTIALS_PATH)\n",
    "\n",
    "PROJECT_ID = \"clgx-gis-app-dev-06e3\"\n",
    "DATASET_ID = \"encumbered_parcels\"\n",
    "FINAL_TABLE_ID = \"wetlands\"\n",
    "GCS_BUCKET = \"geospatial-projects\"\n",
    "GCS_PREFIX = \"infra_parcels/wetlands_v2/county/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66564a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialization ---\n",
    "# This script assumes you have authenticated with GCP CLI using:\n",
    "# gcloud auth application-default login\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9795505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring final table `wetlands` exists...\n",
      "Table check complete.\n"
     ]
    }
   ],
   "source": [
    "# This step ensures the target table is ready before the loop starts.\n",
    "# The schema is defined to match the output of the append query.\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `{PROJECT_ID}.{DATASET_ID}.{FINAL_TABLE_ID}` (\n",
    "    NWI_ID STRING,\n",
    "    state STRING,\n",
    "    fips STRING,\n",
    "    county STRING,\n",
    "    county_full_name STRING,\n",
    "    ATTRIBUTE STRING,\n",
    "    WETLAND_TYPE STRING,\n",
    "    ACRES FLOAT64,\n",
    "    SUBSYSTEM_NAME STRING,\n",
    "    CLASS_NAME STRING,\n",
    "    SUBCLASS_NAME STRING,\n",
    "    SPLIT_CLASS_NAME STRING,\n",
    "    WATER_REGIME_NAME STRING,\n",
    "    WATER_REGIME_SUBGROUP STRING,\n",
    "    geometry GEOGRAPHY\n",
    ")\n",
    "CLUSTER BY state, fips, geometry;\n",
    "\"\"\"\n",
    "print(f\"Ensuring final table `{FINAL_TABLE_ID}` exists...\")\n",
    "create_job = bq_client.query(create_table_sql)\n",
    "create_job.result() # Wait for the create operation to complete\n",
    "print(\"Table check complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492e119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Resumable BigQuery Upload Process ---\n",
      "Found 2723 total files in GCS.\n",
      "Found 0 files already processed.\n",
      "-> 2723 new files to process.\n",
      "\n",
      "Processing file 1 of 2723: gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet\n",
      "  SUCCESS: Loaded gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet to temp table.\n",
      "  ERROR: Failed to process gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet.\n",
      "  REASON: No matching signature for function SAFE_CONVERT_BYTES_TO_STRING\n",
      "  Argument types: STRING\n",
      "  Signature: SAFE_CONVERT_BYTES_TO_STRING(BYTES)\n",
      "    Argument 1: Unable to coerce type STRING to expected type BYTES at [7:38]\n",
      "  INFO: Cleaned up temp table temp_load_0_AK_02013_wetlands.\n",
      "\n",
      "Processing file 2 of 2723: gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00000-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet\n",
      "  SUCCESS: Loaded gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00000-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet to temp table.\n",
      "  ERROR: Failed to process gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00000-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet.\n",
      "  REASON: No matching signature for function SAFE_CONVERT_BYTES_TO_STRING\n",
      "  Argument types: GEOGRAPHY\n",
      "  Signature: SAFE_CONVERT_BYTES_TO_STRING(BYTES)\n",
      "    Argument 1: Unable to coerce type GEOGRAPHY to expected type BYTES at [7:38]\n",
      "  INFO: Cleaned up temp table temp_load_1_part-00000-b3b40955-611d-4032-be73-be012b8be1d0-c000.\n",
      "\n",
      "Processing file 3 of 2723: gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00001-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet\n",
      "  SUCCESS: Loaded gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00001-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet to temp table.\n",
      "  ERROR: Failed to process gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00001-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet.\n",
      "  REASON: No matching signature for function SAFE_CONVERT_BYTES_TO_STRING\n",
      "  Argument types: GEOGRAPHY\n",
      "  Signature: SAFE_CONVERT_BYTES_TO_STRING(BYTES)\n",
      "    Argument 1: Unable to coerce type GEOGRAPHY to expected type BYTES at [7:38]\n",
      "  INFO: Cleaned up temp table temp_load_2_part-00001-b3b40955-611d-4032-be73-be012b8be1d0-c000.\n",
      "\n",
      "Processing file 4 of 2723: gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00002-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet\n",
      "  SUCCESS: Loaded gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00002-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet to temp table.\n",
      "  ERROR: Failed to process gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00002-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet.\n",
      "  REASON: No matching signature for function SAFE_CONVERT_BYTES_TO_STRING\n",
      "  Argument types: GEOGRAPHY\n",
      "  Signature: SAFE_CONVERT_BYTES_TO_STRING(BYTES)\n",
      "    Argument 1: Unable to coerce type GEOGRAPHY to expected type BYTES at [7:38]\n",
      "  INFO: Cleaned up temp table temp_load_3_part-00002-b3b40955-611d-4032-be73-be012b8be1d0-c000.\n",
      "\n",
      "Processing file 5 of 2723: gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00003-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet\n",
      "  SUCCESS: Loaded gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00003-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet to temp table.\n",
      "  ERROR: Failed to process gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00003-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet.\n",
      "  REASON: No matching signature for function SAFE_CONVERT_BYTES_TO_STRING\n",
      "  Argument types: GEOGRAPHY\n",
      "  Signature: SAFE_CONVERT_BYTES_TO_STRING(BYTES)\n",
      "    Argument 1: Unable to coerce type GEOGRAPHY to expected type BYTES at [7:38]\n",
      "  INFO: Cleaned up temp table temp_load_4_part-00003-b3b40955-611d-4032-be73-be012b8be1d0-c000.\n",
      "\n",
      "Processing file 6 of 2723: gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00004-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet\n",
      "  SUCCESS: Loaded gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00004-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet to temp table.\n",
      "  ERROR: Failed to process gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00004-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet.\n",
      "  REASON: No matching signature for function SAFE_CONVERT_BYTES_TO_STRING\n",
      "  Argument types: GEOGRAPHY\n",
      "  Signature: SAFE_CONVERT_BYTES_TO_STRING(BYTES)\n",
      "    Argument 1: Unable to coerce type GEOGRAPHY to expected type BYTES at [7:38]\n",
      "  INFO: Cleaned up temp table temp_load_5_part-00004-b3b40955-611d-4032-be73-be012b8be1d0-c000.\n",
      "\n",
      "Processing file 7 of 2723: gs://geospatial-projects/infra_parcels/wetlands_v2/county/AK_02013_wetlands.parquet/part-00005-b3b40955-611d-4032-be73-be012b8be1d0-c000.snappy.parquet\n",
      "  INFO: Cleaned up temp table temp_load_6_part-00005-b3b40955-611d-4032-be73-be012b8be1d0-c000.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_OperationNotComplete\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\future\\polling.py:120\u001b[0m, in \u001b[0;36mPollingFuture._done_or_raise\u001b[1;34m(self, retry)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone(retry\u001b[38;5;241m=\u001b[39mretry):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _OperationNotComplete()\n",
      "\u001b[1;31m_OperationNotComplete\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 55\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Step 1: Load the single file into a temporary BQ table.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     load_job \u001b[38;5;241m=\u001b[39m bq_client\u001b[38;5;241m.\u001b[39mload_table_from_uri(\n\u001b[0;32m     53\u001b[0m         gcs_uri, temp_table_ref, job_config\u001b[38;5;241m=\u001b[39mjob_config\n\u001b[0;32m     54\u001b[0m     )\n\u001b[1;32m---> 55\u001b[0m     load_job\u001b[38;5;241m.\u001b[39mresult()  \u001b[38;5;66;03m# Wait for the job to complete\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  SUCCESS: Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgcs_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to temp table.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Step 2: Append the data to the final table, handling geometry conversion.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\base.py:1001\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[1;34m(self, retry, timeout)\u001b[0m\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1000\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m\"\u001b[39m: retry}\n\u001b[1;32m-> 1001\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(_AsyncJob, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\future\\polling.py:256\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[1;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_DEFAULT_VALUE, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, polling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the result of the operation.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m    This method will poll for operation status periodically, blocking if\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m            the timeout is reached before the operation completes.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\future\\polling.py:137\u001b[0m, in \u001b[0;36mPollingFuture._blocking_poll\u001b[1;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[0;32m    134\u001b[0m     polling \u001b[38;5;241m=\u001b[39m polling\u001b[38;5;241m.\u001b[39mwith_timeout(timeout)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     polling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done_or_raise)(retry\u001b[38;5;241m=\u001b[39mretry)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRetryError:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mTimeoutError(\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation did not complete within the designated timeout of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolling\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eprashar\\AppData\\Local\\miniforge3\\envs\\ip_dev\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:164\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m         _retry_error_helper(\n\u001b[0;32m    154\u001b[0m             exc,\n\u001b[0;32m    155\u001b[0m             deadline,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m             timeout,\n\u001b[0;32m    162\u001b[0m         )\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(sleep)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleep generator stopped yielding sleep values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Log files to track progress and failures\n",
    "PROCESSED_LOG_FILE = \"processed_files.log\"\n",
    "FAILED_LOG_FILE = \"failed_files.log\"\n",
    "\n",
    "# --- Initialization ---\n",
    "# This script assumes you have authenticated with GCP CLI using:\n",
    "# gcloud auth application-default login\n",
    "\n",
    "def get_processed_files(log_file):\n",
    "    \"\"\"Reads a log file and returns a set of GCS URIs that have been successfully processed.\"\"\"\n",
    "    if not os.path.exists(log_file):\n",
    "        return set()\n",
    "    with open(log_file, \"r\") as f:\n",
    "        # Read lines and strip any whitespace/newlines\n",
    "        return set(line.strip() for line in f if line.strip())\n",
    "\n",
    "def get_files_to_process(bucket_name, prefix, processed_files_set):\n",
    "    \"\"\"Lists files in GCS, filtering out those already processed.\"\"\"\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
    "    all_files = [f\"gs://{bucket_name}/{blob.name}\" for blob in blobs if blob.name.endswith(\".parquet\")]\n",
    "    \n",
    "    files_to_process = [f for f in all_files if f not in processed_files_set]\n",
    "    print(f\"Found {len(all_files)} total files in GCS.\")\n",
    "    print(f\"Found {len(processed_files_set)} files already processed.\")\n",
    "    print(f\"-> {len(files_to_process)} new files to process.\")\n",
    "    return files_to_process\n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "print(\"--- Starting Resumable BigQuery Upload Process ---\")\n",
    "\n",
    "# 1. Get the set of files that have already been processed successfully.\n",
    "processed_files = get_processed_files(PROCESSED_LOG_FILE)\n",
    "\n",
    "# 2. Get the list of all source files in GCS, excluding the ones we've already done.\n",
    "files_to_process = get_files_to_process(GCS_BUCKET, GCS_PREFIX, processed_files)\n",
    "\n",
    "# 3. Loop over the remaining files and upload them one by one.\n",
    "for i, gcs_uri in enumerate(files_to_process):\n",
    "    print(f\"\\nProcessing file {i+1} of {len(files_to_process)}: {gcs_uri}\")\n",
    "    \n",
    "    # Use a temporary table for each file load to make the process atomic\n",
    "    temp_table_id = f\"temp_load_{i}_{os.path.basename(gcs_uri).split('.')[0]}\"\n",
    "    temp_table_ref = bq_client.dataset(DATASET_ID).table(temp_table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.PARQUET,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load the single file into a temporary BQ table.\n",
    "        load_job = bq_client.load_table_from_uri(\n",
    "            gcs_uri, temp_table_ref, job_config=job_config\n",
    "        )\n",
    "        load_job.result()  # Wait for the job to complete\n",
    "        print(f\"  SUCCESS: Loaded {gcs_uri} to temp table.\")\n",
    "\n",
    "        # Step 2: Append the data to the final table, handling geometry conversion.\n",
    "        append_sql = f\"\"\"\n",
    "            INSERT INTO `{PROJECT_ID}.{DATASET_ID}.{FINAL_TABLE_ID}`\n",
    "            SELECT\n",
    "                * EXCEPT (geometry),\n",
    "                -- This CASE statement robustly handles both WKB and GeoJSON\n",
    "                CASE\n",
    "                    WHEN STARTS_WITH(SAFE_CONVERT_BYTES_TO_STRING(geometry), '{{')\n",
    "                        THEN ST_GEOGFROMGEOJSON(SAFE_CONVERT_BYTES_TO_STRING(geometry), make_valid => TRUE)\n",
    "                    ELSE ST_GEOGFROMWKB(geometry)\n",
    "                END AS geometry\n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.{temp_table_id}`;\n",
    "        \"\"\"\n",
    "        append_job = bq_client.query(append_sql)\n",
    "        append_job.result() # Wait for the append to finish\n",
    "        print(f\"  SUCCESS: Appended data to final table.\")\n",
    "\n",
    "        # Step 3: If both steps succeed, log the file as processed.\n",
    "        with open(PROCESSED_LOG_FILE, \"a\") as f:\n",
    "            f.write(f\"{gcs_uri}\\n\")\n",
    "        print(f\"  SUCCESS: Logged {gcs_uri} as completed.\")\n",
    "\n",
    "    except GoogleCloudError as e:\n",
    "        # If the load fails, log the error and the file name, then continue.\n",
    "        print(f\"  ERROR: Failed to process {gcs_uri}.\")\n",
    "        error_message = e.errors[0]['message'] if e.errors else str(e)\n",
    "        print(f\"  REASON: {error_message}\")\n",
    "        with open(FAILED_LOG_FILE, \"a\") as f:\n",
    "            f.write(f\"{gcs_uri}\\t{error_message}\\n\")\n",
    "            \n",
    "    finally:\n",
    "        # Clean up the temporary table regardless of success or failure\n",
    "        bq_client.delete_table(temp_table_ref, not_found_ok=True)\n",
    "        print(f\"  INFO: Cleaned up temp table {temp_table_id}.\")\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
